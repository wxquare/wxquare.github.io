<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"yoursite.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="wxquare&#39;s Blogs">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="wxquare&#39;s Blogs">
<meta property="og:locale">
<meta property="article:author" content="wxquare">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://yoursite.com/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>wxquare's Blogs</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">wxquare's Blogs</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-friends"><a href="/friends" rel="section"><i class="fa fa-user fa-fw"></i>Friends</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="wxquare"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">wxquare</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/xianguiwang0316@gmail.com" title="E-Mail → xianguiwang0316@gmail.com" rel="noopener me"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/TVM-Graph-optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/TVM-Graph-optimization/" class="post-title-link" itemprop="url">TVM学习笔记--了解Relay和图优化</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:54" itemprop="dateModified" datetime="2025-08-28T16:59:54+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>　　TVM主要包括两个部分，一个是Relay和图优化(graph-level)，另一个就是算子（operator）级别优化，这里简单写最近了解到的关于relay和图优化方面的东西。我们都知道深度学习网络通常都是通过计算图来描述的，计算图中的节点表示各种同的算子(opertor),边表示算子之间的依赖关系。Relay可以理解为一种可以描述深度学习网络的函数式编程语言，通过relay可以描述复杂的深度网络，文中提到了control flow。最近一段时间的时间学习直观的感受的Relay编写网络模型和其它框架没什么太多的区别，但是提供的文本形式的中间表示，对开发和调试有很大的帮助。另外，它提供了许多用于图优化的pass，供大家学习和参考。测试代码都在0.6版本上调试通过。<br>    代码地址：<a href="https://github.com/wxquare/programming/tree/master/blog/TVM_graph_optimization">https://github.com/wxquare/programming/tree/master/blog/TVM_graph_optimization</a></p>
<h2 id="一、Hello-Relay"><a href="#一、Hello-Relay" class="headerlink" title="一、Hello Relay"></a>一、Hello Relay</h2><p>既然Relay是一种可以描述计算的函数式语言，逛社区的发现一段代码，可以当作Relay的第一个程序。<br>API参考:<a href="https://docs.tvm.ai/api/python/relay/index.html">https://docs.tvm.ai/api/python/relay/index.html</a></p>
<pre><code><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from tvm import relay</span><br><span class="line">import tvm.relay.op</span><br><span class="line"></span><br><span class="line">x = relay.expr.var(&#x27;x&#x27;, relay.scalar_type(&#x27;int64&#x27;), dtype = &#x27;int64&#x27;)</span><br><span class="line">one = relay.expr.const(1, dtype = &#x27;int64&#x27;)</span><br><span class="line">add = relay.op.tensor.add(x, one)    </span><br><span class="line">func = relay.expr.Function([x], add, relay.scalar_type(&#x27;int64&#x27;))</span><br><span class="line"></span><br><span class="line">mod = relay.Module.from_expr(func)  # note this API</span><br><span class="line">print(&quot;Relay module function:\n&quot;, mod.astext(show_meta_data=False))</span><br><span class="line">graph, lib, params = tvm.relay.build(mod, &#x27;llvm&#x27;, params=&#123;&#125;)</span><br><span class="line">print(&quot;TVM graph:\n&quot;, graph)</span><br><span class="line">print(&quot;TVM parameters:\n&quot;, params)</span><br><span class="line">print(&quot;TVM compiled target function:\n&quot;, lib.get_source())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</code></pre>
<h2 id="二、使用Relay定义卷积单元"><a href="#二、使用Relay定义卷积单元" class="headerlink" title="二、使用Relay定义卷积单元"></a>二、使用Relay定义卷积单元</h2><p>在学习Relay的时候参考了<a href="https://zhuanlan.zhihu.com/p/91283238">https://zhuanlan.zhihu.com/p/91283238</a> 这篇文章。但是可能因为版本的问题，很多API多不兼容了，因此修改了一些地方，建议读者也可以去看一下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">import tvm</span><br><span class="line">from tvm.relay import transform</span><br><span class="line">import tvm.relay as relay</span><br><span class="line">import numpy as np</span><br><span class="line">from tvm.contrib import graph_runtime</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def batch_norm_infer(data,</span><br><span class="line">                    gamma=None,</span><br><span class="line">                    beta=None,</span><br><span class="line">                    moving_mean=None,</span><br><span class="line">                    moving_var=None,</span><br><span class="line">                    **kwargs):</span><br><span class="line">    name = kwargs.get(&quot;name&quot;)</span><br><span class="line">    kwargs.pop(&quot;name&quot;)</span><br><span class="line">    if not gamma:</span><br><span class="line">        gamma = relay.var(name + &quot;_gamma&quot;)</span><br><span class="line">    if not beta:</span><br><span class="line">        beta = relay.var(name + &quot;_beta&quot;)</span><br><span class="line">    if not moving_mean:</span><br><span class="line">        moving_mean = relay.var(name + &quot;_moving_mean&quot;)</span><br><span class="line">    if not moving_var:</span><br><span class="line">        moving_var = relay.var(name + &quot;_moving_var&quot;)</span><br><span class="line">    return relay.nn.batch_norm(data,</span><br><span class="line">                            gamma=gamma,</span><br><span class="line">                            beta=beta,</span><br><span class="line">                            moving_mean=moving_mean,</span><br><span class="line">                            moving_var=moving_var,</span><br><span class="line">                            **kwargs)[0]</span><br><span class="line"></span><br><span class="line">def conv2d(data, weight=None, **kwargs):</span><br><span class="line">    name = kwargs.get(&quot;name&quot;)</span><br><span class="line">    kwargs.pop(&quot;name&quot;)</span><br><span class="line">    if not weight:</span><br><span class="line">        weight = relay.var(name + &quot;_weight&quot;)</span><br><span class="line">    return relay.nn.conv2d(data, weight, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def conv_block(data, name, channels, kernel_size=(3, 3), strides=(1, 1),</span><br><span class="line">            padding=(1, 1), epsilon=1e-5):</span><br><span class="line">    conv = conv2d(</span><br><span class="line">        data=data,</span><br><span class="line">        channels=channels,</span><br><span class="line">        kernel_size=kernel_size,</span><br><span class="line">        strides=strides,</span><br><span class="line">        padding=padding,</span><br><span class="line">        data_layout=&#x27;NCHW&#x27;,</span><br><span class="line">        name=name+&#x27;_conv&#x27;)</span><br><span class="line">    bn = batch_norm_infer(data=conv, epsilon=epsilon, name=name + &#x27;_bn&#x27;)</span><br><span class="line">    act = relay.nn.relu(data=bn)</span><br><span class="line">    return act</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_shape = (1, 3, 224, 224)</span><br><span class="line">kernel_shape = (32, 3, 3, 3)</span><br><span class="line">dtype = &quot;float32&quot;</span><br><span class="line">data = relay.var(&quot;data&quot;, shape=data_shape, dtype=dtype)</span><br><span class="line">act = conv_block(data, &quot;graph&quot;, 32, strides=(2, 2))</span><br><span class="line">func = relay.Function(relay.analysis.free_vars(act),act)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mod = relay.Module.from_expr(func)</span><br><span class="line">mod = relay.transform.InferType()(mod)</span><br><span class="line">shape_dict = &#123;</span><br><span class="line">    v.name_hint : v.checked_type for v in mod[&quot;main&quot;].params&#125;</span><br><span class="line">np.random.seed(0)</span><br><span class="line">params = &#123;&#125;</span><br><span class="line">for k, v in shape_dict.items():</span><br><span class="line">    if k == &quot;data&quot;:</span><br><span class="line">        continue</span><br><span class="line">    init_value = np.random.uniform(-1, 1, v.concrete_shape).astype(v.dtype)</span><br><span class="line">    params[k] = tvm.nd.array(init_value, ctx=tvm.cpu(0))</span><br><span class="line"></span><br><span class="line">target = &quot;llvm&quot;</span><br><span class="line">ctx = tvm.context(target, 0)</span><br><span class="line">print(&quot;Relay module function:\n&quot;, mod.astext(show_meta_data=False))</span><br><span class="line">print(&quot;TVM parameters:\n&quot;, params.keys())</span><br><span class="line"></span><br><span class="line">with relay.build_config(opt_level=3):</span><br><span class="line">    graph, lib, params = relay.build(mod, target, params=params)</span><br><span class="line"></span><br><span class="line">print(&quot;TVM graph:\n&quot;, graph)</span><br><span class="line">print(&quot;TVM parameters:\n&quot;, params.keys())</span><br><span class="line"># print(&quot;TVM compiled target function:\n&quot;, lib.get_source())</span><br><span class="line">module = graph_runtime.create(graph, lib, ctx)</span><br><span class="line">data_tvm = tvm.nd.array((np.random.uniform(-1, 1, size=data_shape)).astype(dtype))</span><br><span class="line">module.set_input(&#x27;data&#x27;, data_tvm)</span><br><span class="line">module.set_input(**params)</span><br><span class="line">module.run()</span><br><span class="line">output = module.get_output(0)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="三、Relay-Graph-Optimization"><a href="#三、Relay-Graph-Optimization" class="headerlink" title="三、Relay Graph Optimization"></a>三、Relay Graph Optimization</h2><p>前面两个例子介绍了怎么使用relay构建网络，这个部分介绍怎么使用relay做图优化。上面例子代码中没有直接图优化的代码，而是包含在relay.build中。通过追踪代码，我们这部分的逻辑集中在 <a href="https://github.com/apache/incubator-tvm/blob/v0.6/src/relay/backend/build_module.cc">https://github.com/apache/incubator-tvm/blob/v0.6/src/relay/backend/build_module.cc</a> 这个文件的optimize函数中。这里罗列了代码用到的pass，relay提供了方便的的文本形式中间描述，感兴趣的可以自己试一下每个pass之后，发生了哪些变化。</p>
<ul>
<li>relay::qnn::transform::Legalize())，这个pass和qnn有关</li>
<li>transform::Legalize()，我理解的这个是和目标有关的优化，一个表达式虽然在语义上等效于另一个，但可以在目标上具有更好的性能。这个在需要在异构环境下生效。</li>
<li>transform::SimplifyInference() 。<br>简化推理阶段的数据流图。在语义上等于输入表达式的简化表达式将被返回。例如将BatchNorm展开以及去掉 dropout。</li>
<li>transform::EliminateCommonSubexpr(fskip))，去除公共子表达式。</li>
<li>transform::CombineParallelConv2D(3)，将多个conv2d运算符合并为一个，这部分优化会将具有相同输入的卷积合并成一个大的卷积运算。</li>
<li>transform::CombineParallelDense(3))，将多个dense运算符组合为一个</li>
<li>transform::FoldConstant()，常量传播优化。</li>
<li>transform::FoldScaleAxis()</li>
<li>transform::CanonicalizeCast()，<br>将特殊运算符规范化为基本运算符。这样可以简化后续分析，例如将bias_add扩展为expand_dims和broadcast_add</li>
<li>transform::CanonicalizeOps()</li>
<li>transform::AlterOpLayout()，layout 变换</li>
<li>transform::FuseOps()，算子融合，根据一些规则，将expr中的运算符融合为较大的运算符。</li>
</ul>
<h2 id="四、使用Python-API-Relay-图优化"><a href="#四、使用Python-API-Relay-图优化" class="headerlink" title="四、使用Python API Relay 图优化"></a>四、使用Python API Relay 图优化</h2><p> TVM核心代码是采用C++编写的，但是也提供了Python接口，这方面初学者体验的使用。Relay图优化核心功能都提供了对应的API，因此可以尝试一下，非常简单。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def my_optimize(func,params=None):</span><br><span class="line"></span><br><span class="line">    if params:</span><br><span class="line">        graph = _bind_params(func, params)</span><br><span class="line"></span><br><span class="line">    # https://docs.tvm.ai/api/python/relay/transform.html</span><br><span class="line">    optimize = relay.transform.Sequential([relay.transform.SimplifyInference(),</span><br><span class="line">                                      relay.transform.FoldConstant(),</span><br><span class="line">                                      relay.transform.FoldScaleAxis(),</span><br><span class="line">                                      relay.transform.CanonicalizeOps(),</span><br><span class="line">                                      relay.transform.FoldConstant()])</span><br><span class="line"></span><br><span class="line">    mod = relay.Module.from_expr(graph)</span><br><span class="line">    mod = optimize(mod)</span><br><span class="line">    return mod[&quot;main&quot;]</span><br><span class="line"></span><br><span class="line">mod[&#x27;main&#x27;] = my_optimize(mod[&#x27;main&#x27;], params)</span><br><span class="line">print(&quot;Relay module function:\n&quot;, mod.astext(show_meta_data=False))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里可以对比优化前后的IR.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Relay module function:</span><br><span class="line"> v0.0.4</span><br><span class="line">def @main(%data: Tensor[(1, 3, 224, 224), float32], %graph_conv_weight: Tensor[(32, 3, 3, 3), float32], %graph_bn_gamma: Tensor[(32), float32], %graph_bn_beta: Tensor[(32), float32], %graph_bn_moving_mean: Tensor[(32), float32], %graph_bn_moving_var: Tensor[(32), float32]) -&gt; Tensor[(1, 32, 112, 112), float32] &#123;</span><br><span class="line">  %0 = nn.conv2d(%data, %graph_conv_weight, strides=[2, 2], padding=[1, 1], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 112, 112), float32] */;</span><br><span class="line">  %1 = nn.batch_norm(%0, %graph_bn_gamma, %graph_bn_beta, %graph_bn_moving_mean, %graph_bn_moving_var) /* ty=(Tensor[(1, 32, 112, 112), float32], Tensor[(32), float32], Tensor[(32), float32]) */;</span><br><span class="line">  %2 = %1.0;</span><br><span class="line">  nn.relu(%2) /* ty=Tensor[(1, 32, 112, 112), float32] */</span><br><span class="line">&#125;</span><br><span class="line"># =====================================</span><br><span class="line">Relay module function:</span><br><span class="line"> v0.0.4</span><br><span class="line">def @main(%data: Tensor[(1, 3, 224, 224), float32]) -&gt; Tensor[(1, 32, 112, 112), float32] &#123;</span><br><span class="line">  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(32, 3, 3, 3), float32] */ /* ty=Tensor[(32, 3, 3, 3), float32] */, strides=[2, 2], padding=[1, 1], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 112, 112), float32] */;</span><br><span class="line">  %1 = multiply(%0, meta[relay.Constant][1] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(1, 32, 112, 112), float32] */;</span><br><span class="line">  %2 = add(%1, meta[relay.Constant][2] /* ty=Tensor[(32, 1, 1), float32] */ /* ty=Tensor[(32, 1, 1), float32] */) /* ty=Tensor[(1, 32, 112, 112), float32] */;</span><br><span class="line">  nn.relu(%2) /* ty=Tensor[(1, 32, 112, 112), float32] */</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// meta data omitted. you can use show_meta_data=True to include meta data</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>参考与进阶学习：<br>[1]. <a href="https://www.zhihu.com/question/331611341/answer/875630325">https://www.zhihu.com/question/331611341/answer/875630325</a><br>[2]. <a href="https://zhuanlan.zhihu.com/p/91283238">https://zhuanlan.zhihu.com/p/91283238</a><br>[3]. <a href="https://docs.tvm.ai/dev/relay_intro.html">https://docs.tvm.ai/dev/relay_intro.html</a><br>[4]. <a href="https://docs.tvm.ai/dev/relay_add_op.html">https://docs.tvm.ai/dev/relay_add_op.html</a><br>[5]. <a href="https://docs.tvm.ai/dev/relay_add_pass.html">https://docs.tvm.ai/dev/relay_add_pass.html</a><br>[6]. <a href="https://arxiv.org/pdf/1810.00952.pdf">https://arxiv.org/pdf/1810.00952.pdf</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/TVM-code-generation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/TVM-code-generation/" class="post-title-link" itemprop="url">TVM学习笔记--代码生成</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:49" itemprop="dateModified" datetime="2025-08-28T16:59:49+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="代码生成的接口"><a href="#代码生成的接口" class="headerlink" title="代码生成的接口"></a>代码生成的接口</h2><p>　　TVM代码生成的接口和主要类型，可以总结为两个build，两个module，两个function。它提供了两个代码生成的接口，tvm.build和tvm.relay.build，前者是针对算子的代码生成，后者是针对relay计算图的代码生成。在0.7版本中，tvm进行了IR的统一，使得两个build的输入参数类型都可以是IRModule，输出类型都是运行时Module。尽管两个build接口统一了输入类型，但是内部包含的函数类型是不一样的，算子编译时是tvm.tir.function.PrimFunc，而relay图编译时函数类型是tvm.relay.function.Function。TVM在设计时提供了方便的调试功能，通过IRModule的astext函数可以查看ir中间描述，通过运行时module的get_source查看生成的代码。下面通过两个简单的例子查看算子和relay图的ir中间描述和以及对应生成的源代码。</p>
<ul>
<li><a href="https://tvm.apache.org/docs/api/python/driver.html?highlight=build#tvm.build">tvm.build</a></li>
<li><a href="https://tvm.apache.org/docs/api/python/relay/index.html?highlight=build#tvm.relay.build">tvm.relay.build</a></li>
<li><a href="https://tvm.apache.org/docs/api/python/ir.html?highlight=irmodule#tvm.ir.IRModule">tvm.ir.module.IRModule</a></li>
<li><a href="https://tvm.apache.org/docs/api/python/runtime.html?highlight=module#tvm.runtime.Module">tvm.runtime.module.Module</a></li>
<li><a href="https://tvm.apache.org/docs/api/python/tir.html?highlight=primfunc#tvm.tir.PrimFunc">tvm.tir.function.PrimFunc</a></li>
<li><a href="https://tvm.apache.org/docs/api/python/relay/index.html?highlight=function#tvm.relay.Function">tvm.relay.function.Function</a></li>
</ul>
<h3 id="算子编译"><a href="#算子编译" class="headerlink" title="算子编译"></a>算子编译</h3><pre><code>import tvm
from tvm import te

M = 1024
K = 1024
N = 1024

# Algorithm
k = te.reduce_axis((0, K), &#39;k&#39;)
A = te.placeholder((M, K), name=&#39;A&#39;)
B = te.placeholder((K, N), name=&#39;B&#39;)
C = te.compute(
           (M, N),
           lambda x, y: te.sum(A[x, k] * B[k, y], axis=k),
           name=&#39;C&#39;)

# Default schedule
s = te.create_schedule(C.op)
ir_m = tvm.lower(s, [A, B, C], simple_mode=True,name=&#39;mmult&#39;)
rt_m = tvm.build(ir_m, [A, B, C], target=&#39;c&#39;, name=&#39;mmult&#39;)

# print tir
print(&quot;tir:\n&quot;, ir_m.astext(show_meta_data=False))
# print source code
print(&quot;source code:\n&quot;,rt_m.get_source())
</code></pre>
<h3 id="relay图编译"><a href="#relay图编译" class="headerlink" title="relay图编译"></a>relay图编译</h3><pre><code>import ssl
ssl._create_default_https_context = ssl._create_unverified_context

from tvm import relay
from tvm.relay import testing
from tvm.contrib import util
import tvm

# Resnet18 workload
resnet18_mod, resnet18_params = relay.testing.resnet.get_workload(num_layers=18)

with relay.build_config(opt_level=0):
    _, resnet18_lib, _ = relay.build_module.build(resnet18_mod, &quot;llvm&quot;, params=resnet18_params)

# print relay ir
print(resnet18_mod.astext(show_meta_data=False))

# print source code
print(resnet18_lib.get_source())
</code></pre>
<h2 id="代码生成的流程"><a href="#代码生成的流程" class="headerlink" title="代码生成的流程"></a>代码生成的流程</h2><p>　　通过上面两个例子我们知道tvm代码生成接口上是IRModule到运行时module的转换，它完成tir或者relay ir到目标target代码的编译，例如c或者llvm IR等。下面的流程图描述整个代码的编译流程，深色表示C++代码，浅色表示python代码。算子编译时会首先进行tir的优化，分离出host和device部分，之后会调用注册的target.build.target函数进行编译。relay图编译相比算子稍微复杂一点，核心代码采用C++开发。它会通过relayBuildModule.Optimize进行relay图优化，之后针对module中的每个lower_funcs进行编译之前，合成最终的运行时module，其后部分的编译流程和算子编译相似。</p>
<p><img src="/images/tvm_code_generation.jpg" alt="TVM代码生成流程"></p>
<h2 id="Codegen的实现"><a href="#Codegen的实现" class="headerlink" title="Codegen的实现"></a>Codegen的实现</h2><p> TVM针对不同的target实现了许多的codgen，它完成了tir到目标代码的翻译工作，例如c,llvm ir等。我们也可以根据需求实现自己的codegen，官网提供了一个<a href="https://tvm.apache.org/docs/dev/relay_bring_your_own_codegen.html">教程</a>。</p>
<ul>
<li>target.build.c</li>
<li>target.build.llvm</li>
<li>target.build.cuda</li>
<li>target.build.opencl</li>
<li>target.build.opengl</li>
<li>target.build.metal</li>
<li>target.build.vulkan</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]. Unified IR RFC,<a href="https://github.com/apache/incubator-tvm/issues/4617">https://github.com/apache/incubator-tvm/issues/4617</a><br>[2]. Codegen的实现：<a href="https://tvm.apache.org/docs/dev/relay_bring_your_own_codegen.html">https://tvm.apache.org/docs/dev/relay_bring_your_own_codegen.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/TVM-hello/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/TVM-hello/" class="post-title-link" itemprop="url">初识TVM，相比于tensorflow的2倍性能提升</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:58" itemprop="dateModified" datetime="2025-08-28T16:59:58+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>　　<br>　　最近在做深度学习模型加速的工作，先后尝试了模型权重量化(quantization)、模型权重稀疏（sparsification）和模型通道剪枝(channel pruning)等压缩方法，但效果都不明显。权重量化和稀疏属于非结构化的压缩，需要推理引擎和硬件的优化才能实现推理加速，通道剪枝能直接减少FLOPs，确实能卷积网络的效率，在ResNet56网络中能大概提升卷积50%的速度。在工程实践中，除了通过模型压缩提升推理性能，还可以通过优化推理引擎提高推理效率。目前存在多种开源的推理引擎，我首先尝试了TVM。</p>
<h3 id="为什么选择TVM"><a href="#为什么选择TVM" class="headerlink" title="为什么选择TVM"></a>为什么选择TVM</h3><p>　　为提升深度学习模型的推理效率，设备平台制造商针对自己的平台推出优化的推理引擎，例如NAVIDA的tensorRT，Intel的OpenVINO，Tencent针对移动端应用推出NCNN等。目前，深度学习模型应用广泛，在服务端和移动端都有应用，甚至于特殊的嵌入式场景想，它们都有加速模型推理的需求。个人感觉针对不同平台选择不同的推理引擎，学习成本太高。我这里选择尝试TVM，主要有以下几个原因：</p>
<ul>
<li>尝试了过一些模型压缩方法，效率提升有限</li>
<li>有些是模型压缩方法需要推理引擎和硬件的支持的，例如量化</li>
<li>tensorflow推理效率有限，需要更好的推理引擎</li>
<li>针对平台选择不同推理引擎，学习成本太高</li>
<li>需要能支持跨平台的推理引擎，未来可能在定制的嵌入式芯片上运行深度学习模型</li>
<li>除了TVM之外，还存在XLA之类方案，选择TVM也是因为tianqi等大佬主导的项目，相信大佬！</li>
</ul>
<h3 id="初次体验TVM，相比于tensorflow2倍的性能提升"><a href="#初次体验TVM，相比于tensorflow2倍的性能提升" class="headerlink" title="初次体验TVM，相比于tensorflow2倍的性能提升"></a>初次体验TVM，相比于tensorflow2倍的性能提升</h3><p>　　看了几篇TVM介绍文章后，了解到它是从深度学习编译器的角度来做推理引擎，目前技术领域还比较新，具体技术细节以后有机会会深入学习，这里主要想体验一下使用TVM做深度模型推理，重点是推理效率的提升，因为是骡子还是马得拉出来遛遛。参考官方文档进行编译安装，整个过程还是比较简单的，结果显示相比于tensorflow大概100%的性能提升。实验环境是ubuntu 19.04，x86_64架构。</p>
<ol>
<li>安装llvm,也可源码编译<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install llvm</span><br></pre></td></tr></table></figure></li>
<li>编译TVM<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git clone --recursive https://github.com/dmlc/tvm.git</span><br><span class="line">$ cd tvm $$ mkdir build</span><br><span class="line">$ cp cmake/config.cmake build</span><br><span class="line"># 编辑config.cmake 然后将USE_LLVM OFF 改为 set(USE_LLVM /usr/bin/llvm-config)</span><br><span class="line">$ cd build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ cmake -j4</span><br></pre></td></tr></table></figure></li>
<li>编辑.bashrc配置Python环境<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export TVM_HOME=/home/xxxx/code/tvm</span><br><span class="line">export PYTHONPATH=$TVM_HOME/python:$TVM_HOME/topi/python:$TVM_HOME/nnvm/python</span><br></pre></td></tr></table></figure></li>
<li>官方<a href="https://docs.tvm.ai/tutorials/frontend/from_tensorflow.html#sphx-glr-tutorials-frontend-from-tensorflow-py">Compile Tensorflow Models</a><br>直接运行出现了两个问题，下载文件时和SSL相关，另外一个是缺少antlr<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># install antlr</span><br><span class="line">$ pip install antlr4-python3-runtime</span><br><span class="line"># debug ssl</span><br><span class="line">import ssl</span><br><span class="line">ssl._create_default_https_context = ssl._create_unverified_context</span><br><span class="line"># run demo</span><br><span class="line">$ python from_tensorflow.py</span><br></pre></td></tr></table></figure></li>
<li>在代码中加入时间测试，实验测试结果。TVM与测试时间为0.277s，tensorflow为0.586s。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">============ TVM ============ 0.2770531177520752</span><br><span class="line">African elephant, Loxodonta africana (score = 0.58335)</span><br><span class="line">tusker (score = 0.33901)</span><br><span class="line">Indian elephant, Elephas maximus (score = 0.02391)</span><br><span class="line">banana (score = 0.00025)</span><br><span class="line">vault (score = 0.00021)</span><br><span class="line">============= Tensorflow ===== 0.58619508743286133</span><br><span class="line">===== TENSORFLOW RESULTS =======</span><br><span class="line">African elephant, Loxodonta africana (score = 0.58394)</span><br><span class="line">tusker (score = 0.33909)</span><br><span class="line">Indian elephant, Elephas maximus (score = 0.03186)</span><br><span class="line">banana (score = 0.00022)</span><br><span class="line">desk (score = 0.00019)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="未填的坑"><a href="#未填的坑" class="headerlink" title="未填的坑"></a>未填的坑</h2><p>　　过程遇到一个坑，查了TVM社区，没有很好的解答，看起来好像会和性能有关，希望路过的大佬能帮忙解决。<a href="https://discuss.tvm.ai/t/cannot-find-config-for-target-llvm-when-using-autotvm-in-tensorflow-example-for-cpu/1544">https://discuss.tvm.ai/t/cannot-find-config-for-target-llvm-when-using-autotvm-in-tensorflow-example-for-cpu/1544</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">WARNING:autotvm:Cannot find config for target=llvm, workload=(&#x27;conv2d&#x27;, (1, 8, 8, 2048, &#x27;float32&#x27;), (1, 1, 2048, 384, &#x27;float32&#x27;), (1, 1), (0, 0), (1, 1), &#x27;NHWC&#x27;, &#x27;float32&#x27;). A fallback configuration is used, which may bring great performance regression.</span><br><span class="line">WARNING:autotvm:Cannot find config for target=llvm, workload=(&#x27;conv2d&#x27;, (1, 8, 8, 2048, &#x27;float32&#x27;), (1, 1, 2048, 448, &#x27;float32&#x27;), (1, 1), (0, 0), (1, 1), &#x27;NHWC&#x27;, &#x27;float32&#x27;). A fallback configuration is used, which may bring great performance regression.</span><br><span class="line">WARNING:autotvm:Cannot find config for target=llvm, workload=(&#x27;conv2d&#x27;, (1, 8, 8, 2048, &#x27;float32&#x27;), (1, 1, 2048, 192, &#x27;float32&#x27;), (1, 1), (0, 0), (1, 1), &#x27;NHWC&#x27;, &#x27;float32&#x27;). A fallback configuration is used, which may bring great performance regression.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>参考：</p>
<ol>
<li>tvm install: <a href="https://docs.tvm.ai/install/from_source.html">https://docs.tvm.ai/install/from_source.html</a></li>
<li>tvm tutorial: <a href="https://docs.tvm.ai/tutorials/frontend/from_tensorflow.html#sphx-glr-tutorials-frontend-from-tensorflow-py">Compile Tensorflow Models</a></li>
<li>未填的坑：<a href="https://discuss.tvm.ai/t/cannot-find-config-for-target-llvm-when-using-autotvm-in-tensorflow-example-for-cpu/1544">https://discuss.tvm.ai/t/cannot-find-config-for-target-llvm-when-using-autotvm-in-tensorflow-example-for-cpu/1544</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/TVM-quantization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/TVM-quantization/" class="post-title-link" itemprop="url">TVM学习笔记--模型量化(int8)及其测试数据</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 17:00:01" itemprop="dateModified" datetime="2025-08-28T17:00:01+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>　　坚持了接近一年的视频算法相关的项目，老板最终还是喊停了。并没有感到特别意外，只是在对一个东西突然有些兴趣或者说入门的时候，戛然而止，多少有些不甘心和遗憾，但是以后会在业余继续学习的，也希望自己在2020年能把工作逐渐聚焦到这块吧。</p>
<p>　　接触TVM到有两个原因。一是需要支持多种优化手段的推理引擎，例如量化、图优化、稀疏优化、模型压缩剪枝等。尝试过在tensorflow的quantization和非结构性剪枝(no-structural pruning)，加速效果非常一般，因为这些优化手段需要推理引擎的支持，但是当时我们都是纯后台出身，也没人掌握这个内容。再之后尝试channel pruning，终于取得了一些进展，但是30%的提升leader并不满意。二是需要支持多种平台的推理引擎，例如NV GPU&#x2F;x86&#x2F;ARM GPU等。由于组内业务迟迟没有好的落地场景，尝试了多种手段，需要的把深度模型部署在不同的平台上。记得有次，花了两周的时间把DaSiamRPN模型移植到终端上。从零开始pytorch、onnx、tflite、android，期间踩了许多的坑，结果在移动端运行需要4秒时间来处理一帧图像。。。期间同事也曾通过tensorRT部署模型，效率反而下降。一次偶然的机会了解到TVM，当时感觉它可能是比较适合我们团队的需求的。</p>
<p>　　由于我之前学习信号处理的，比较容易理解量化。模型量化quantization也在深度学习在部署落地时提高效率的常用的方法。之前有写过关于<a href="https://zhuanlan.zhihu.com/p/86440423">tensorflow模型量化</a>的方法，写得不好，对于想学习模型量化知识的可以参考下面链接进行学习：</p>
<p><strong>模型量化相关：</strong><br>【1】<a href="https://jackwish.net/2019/neural-network-quantization-introduction-chn.html">神经网络量化简介</a><br>【2】<a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">Tensort量化:8-bit-inference-with-tensort</a><br>【3】<a href="http://www.ruanyifeng.com/blog/2010/06/ieee_floating-point_representation.html">阮一峰：浮点数的二进制表示</a><br>【4】<a href="https://arxiv.org/pdf/1806.08342.pdf">Quantizing deep convolutional networks for efficient inference</a></p>
<p><strong>TVM量化相关RFC</strong><br>【INT8 quantization proposal】：<a href="https://discuss.tvm.ai/t/int8-quantization-proposal/516%EF%BC%882018.02.02%EF%BC%89">https://discuss.tvm.ai/t/int8-quantization-proposal/516（2018.02.02）</a><br>【TVM quantizationRFC】 <a href="https://github.com/apache/incubator-tvm/issues/2259(2018.12.09)">https://github.com/apache/incubator-tvm/issues/2259(2018.12.09)</a></p>
<p>　　目前，官网上还没有关于模型量化的教程和文档，对于刚接触新手来说可能有些麻烦，这里提供提供一个参考代码，方便新手学习。除此之外，也测试了TVM的int8量化性能，结果显示TVM的量化加速效果不是很好，甚至略有下降，需要配合autotvm一起使用。<a href="https://github.com/wxquare/programming/tree/master/blog/TVM_quantization">测试代码地址</a>。测试结果如下，希望对大家了解TVM有帮助。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>原始框架</th>
<th>原始框架运行时间</th>
<th>TVM FP32</th>
<th>TVM int8</th>
<th>TVM int8+AutoTVM</th>
</tr>
</thead>
<tbody><tr>
<td>resnet18v1</td>
<td>mxnet 1.5.1</td>
<td>27.8ms</td>
<td>46.9ms</td>
<td>51.10ms</td>
<td>25.83ms</td>
</tr>
<tr>
<td>Inceptionv1</td>
<td>tensorflow 1.13</td>
<td>560ms</td>
<td>164ms</td>
<td>185ms</td>
<td>116ms</td>
</tr>
</tbody></table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/TVM-tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/TVM-tutorial/" class="post-title-link" itemprop="url">TVM 学习资料整理（持续更新）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 17:00:04" itemprop="dateModified" datetime="2025-08-28T17:00:04+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>这周没什么产出，在TVM社区闲逛。。。</strong></p>
<h3 id="1-TVM编译和安装"><a href="#1-TVM编译和安装" class="headerlink" title="1.TVM编译和安装"></a>1.TVM编译和安装</h3><ul>
<li><a href="https://wxquare.github.io/2019/10/24/other/TVM-hello/">https://wxquare.github.io/2019/10/24/other/TVM-hello/</a></li>
</ul>
<h3 id="2-TVM中向量相加"><a href="#2-TVM中向量相加" class="headerlink" title="2.TVM中向量相加"></a>2.TVM中向量相加</h3><ul>
<li><a href="http://tvm.d2l.ai/chapter_getting_started/vector_add.html">http://tvm.d2l.ai/chapter_getting_started/vector_add.html</a></li>
</ul>
<h3 id="3-TVM编译tensorflow模型"><a href="#3-TVM编译tensorflow模型" class="headerlink" title="3.TVM编译tensorflow模型"></a>3.TVM编译tensorflow模型</h3><ul>
<li><a href="https://wxquare.github.io/2019/10/24/other/TVM-hello/">https://wxquare.github.io/2019/10/24/other/TVM-hello/</a></li>
<li><a href="https://docs.tvm.ai/tutorials/frontend/from_tensorflow.html#sphx-glr-tutorials-frontend-from-tensorflow-py">https://docs.tvm.ai/tutorials/frontend/from_tensorflow.html#sphx-glr-tutorials-frontend-from-tensorflow-py</a></li>
</ul>
<h3 id="4-TVM怎么做模型量化？-doing"><a href="#4-TVM怎么做模型量化？-doing" class="headerlink" title="4.TVM怎么做模型量化？(doing)"></a>4.TVM怎么做模型量化？(doing)</h3><ul>
<li><a href="https://discuss.tvm.ai/t/int8-quantization-proposal/516">https://discuss.tvm.ai/t/int8-quantization-proposal/516</a></li>
<li><a href="https://discuss.tvm.ai/t/quantization-story/3920">https://discuss.tvm.ai/t/quantization-story/3920</a></li>
</ul>
<p>参考：<br>【1】 [Dive into Deep Learning Compiler](Dive into Deep Learning Compiler “<a href="http://tvm.d2l.ai/">http://tvm.d2l.ai/</a>“)<br>【2】 <a href="https://tvm.ai/">https://tvm.ai/</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/tensorflow-benchmark-model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/tensorflow-benchmark-model/" class="post-title-link" itemprop="url">了解tensorflow中的模型基准测试工具</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:33" itemprop="dateModified" datetime="2025-08-28T16:59:33+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>　　深度学习模型落地需要考虑决定推理（inference）过程所需的计算资源（成本）和效率（系统的吞吐量和延时），有时甚至需要进行适当的模型裁剪和压缩工作。理论上说，模型结构一旦确定是可以计算它的复杂度和计算量，但这有些繁琐。实际中可以借助一些工具帮助预估模型实际的性能，比较模型优化前后的差别，主要使用到的是benchmark_model和summarize_graph。</p>
<h2 id="一、benchmark-model模型推理速度分析"><a href="#一、benchmark-model模型推理速度分析" class="headerlink" title="一、benchmark_model模型推理速度分析"></a>一、benchmark_model模型推理速度分析</h2><p>　　在深度学习模型工程落地时，我们追求在成本可控的前提下提高良好的用户体验，因此模型的推理效率和计算代价是重要的衡量指标。通常用FLOPs（floating point operations）描述模型的计算力消耗，它表示浮点运算计算量，用来衡量算法&#x2F;模型的复杂度。我们是可以从原理上计算出模型需要的FLOPs，参考：<a href="https://www.zhihu.com/question/65305385%E3%80%82">https://www.zhihu.com/question/65305385。</a> 除了从理论计算之外，还可以使用tensorflow中的 benchmark_model 工具来进行粗略估计，它可以帮助估算出模型所需的浮点操作数(FLOPS)，然后你就可以使用这些信息来确定你的模型在你的目标设备上运行的可行性。除此之外，比较容易混淆的概念是FLOPS（floating point operations per second），意指每秒浮点运算次数，理解为计算速度，它是衡量硬件性能的指标对于来说TESLA P40可以每秒处理12T个FLOP，普通单核CPU每秒大概处理100亿次的FLOP。当有了计算操作消耗的估计之后，它就对你计划的目标设备上有所帮助，如果模型的计算操作太多，那么就需要优化模型减小FLOP数量。</p>
<p>　　例如下面的例子中，我们通过benchmark_model分析resetNet20-cifar10，大概有82.15M的FLOPs，该机器每秒执行21.89B，因此该模型大概需要4ms的计算时间。在使用benchmark_model之前，需要使用tensorflow源码进行编译。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">编译benchmark_model</span><br><span class="line">$ bazel build -c opt tensorflow/tools/benchmark:benchmark_model</span><br><span class="line">$ bazel-bin/tensorflow/tools/benchmark/benchmark_model \</span><br><span class="line">--graph=model_original.pb \</span><br><span class="line">--input_layer=&quot;net_input&quot; \</span><br><span class="line">--input_layer_shape=&quot;1,32,32,3&quot; \</span><br><span class="line">--input_layer_type=&quot;float&quot; \</span><br><span class="line">--output_layer=&quot;net_output&quot; \</span><br><span class="line">--show_flops=true \</span><br><span class="line">--show_run_order=false \</span><br><span class="line">--show_time=false \</span><br><span class="line">--num_threads=1</span><br></pre></td></tr></table></figure>


<h4 id="预估FLOPs"><a href="#预估FLOPs" class="headerlink" title="预估FLOPs"></a>预估FLOPs</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-10-11 21:30:31.288678: I tensorflow/tools/benchmark/benchmark_model.cc:636] FLOPs estimate: 82.15M</span><br><span class="line">2019-10-11 21:30:31.288744: I tensorflow/tools/benchmark/benchmark_model.cc:638] FLOPs/second: 21.89B</span><br></pre></td></tr></table></figure>


<h4 id="查看不同类型节点消耗的时间："><a href="#查看不同类型节点消耗的时间：" class="headerlink" title="查看不同类型节点消耗的时间："></a>查看不同类型节点消耗的时间：</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">========================= Summary by node type ==========================================</span><br><span class="line"> [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]</span><br><span class="line">          &lt;&gt;	       65	     4.110	    47.269%	    47.269%	     0.000	       65</span><br><span class="line">FusedBatchNorm	       19	     2.028	    23.324%	    70.592%	   240.384	       19</span><br><span class="line">      Conv2D	       22	     2.003	    23.036%	    93.629%	   868.352	       22</span><br><span class="line">         Pad	        2	     0.239	     2.749%	    96.377%	   115.456	        2</span><br><span class="line">        Relu	       19	     0.082	     0.943%	    97.320%	     0.000	       19</span><br><span class="line">       Const	       65	     0.071	     0.817%	    98.137%	     0.000	       65</span><br><span class="line">        NoOp	        1	     0.066	     0.759%	    98.896%	     0.000	        1</span><br><span class="line">         Add	        9	     0.059	     0.679%	    99.574%	     0.000	        9</span><br><span class="line">        Mean	        1	     0.010	     0.115%	    99.689%	     0.256	        1</span><br><span class="line">     Softmax	        1	     0.008	     0.092%	    99.781%	     0.000	        1</span><br><span class="line">_FusedMatMul	        1	     0.007	     0.081%	    99.862%	     0.040	        1</span><br><span class="line">     _Retval	        1	     0.005	     0.058%	    99.919%	     0.000	        1</span><br><span class="line">     Squeeze	        1	     0.005	     0.058%	    99.977%	     0.000	        1</span><br><span class="line">        _Arg	        1	     0.002	     0.023%	   100.000%	     0.000	        1</span><br><span class="line"></span><br><span class="line">Timings (microseconds): count=1000 first=7287 curr=7567 min=7198 max=18864 avg=8794.03 std=1249</span><br><span class="line">Memory (bytes): count=1000 curr=1224488(all same)</span><br></pre></td></tr></table></figure>

<ul>
<li>node type：进行操作的节点类型。</li>
<li>start：运算符的启动时间，展示了其在操作顺序中的位置。</li>
<li>first: 以毫秒为单位。默认情况下 TensorFlow 会执行 20 次运行结果来获得统计数据，这个字段则表示第一次运行基准测试所需的操作时间。</li>
<li>avg ms：以毫秒为单位。表示整个运行的平均操作时间。</li>
<li>%：一次运行占总运行时间的百分比。这对理解密集计算区域非常有用。</li>
<li>cdf%：整个过程中表格中当前运算符及上方全部运算符的累积计算时间。这对理解神经网络不同层之间的性能分布非常重要，有助于查看是否只有少数节点占用大部分时间。</li>
<li>mem KB：当前层消耗的内存大小。</li>
<li>Name：节点名称。</li>
</ul>
<h2 id="二、summarize-graph-模型大小分析"><a href="#二、summarize-graph-模型大小分析" class="headerlink" title="二、summarize_graph 模型大小分析"></a>二、summarize_graph 模型大小分析</h2><p>　　服务端深度模型落地时主要关注模型的预测效率，移动端模型落地需要考虑模型的大小。通过summarize_graph工具可以帮助我们简要分析模型的参数量和包含哪些op。设置–print_structure&#x3D;true可以观察到模型的结构，这也可以通过tensorboard来可视化实现。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensorflow-1.14.0编译summarize_graph工具</span><br><span class="line">$ bazel build -c opt tensorflow/tools/graph_transforms:summarize_graph</span><br><span class="line">$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph \</span><br><span class="line">--in_graph=reset20_cifar10_original.pb \</span><br><span class="line">--print_structure=true</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Found 1 possible inputs: (name=net_input, type=float(1), shape=[?,32,32,3]) </span><br><span class="line">No variables spotted.</span><br><span class="line">Found 1 possible outputs: (name=net_output, op=Softmax) </span><br><span class="line">Found 272572 (272.57k) const parameters, 0 (0) variable parameters, and 0 control_edges</span><br><span class="line">Op types used: 194 Const, 77 Identity, 22 Conv2D, 19 Relu, 19 FusedBatchNorm, 11 Add, 6 Slice, 5 Pad, 5 Reshape, 4 Sub, 4 MatchingFiles, 3 Switch, 2 Squeeze, 2 ShuffleDataset, 2 ShuffleAndRepeatDataset, 2 StridedSlice, 2 Shape, 2 TensorSliceDataset, 2 RealDiv, 2 PrefetchDataset, 2 ParallelMapDataset, 2 ParallelInterleaveDataset, 2 Transpose, 2 OneHot, 2 BatchDatasetV2, 2 Cast, 2 Maximum, 2 DecodeRaw, 1 GreaterEqual, 1 All, 1 Assert, 1 BiasAdd, 1 Softmax, 1 ExpandDims, 1 FixedLengthRecordDataset, 1 FloorMod, 1 Mul, 1 ReverseV2, 1 Less, 1 MatMul, 1 RandomUniformInt, 1 RandomUniform, 1 Mean, 1 Placeholder, 1 Merge</span><br></pre></td></tr></table></figure>


<p><a href="https://tensorflow.juejin.im/mobile/optimizing.html">https://tensorflow.juejin.im/mobile/optimizing.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/tensorflow-how-to-freeze-model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/tensorflow-how-to-freeze-model/" class="post-title-link" itemprop="url">了解tensorflow不同格式的模型及其转换方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:36" itemprop="dateModified" datetime="2025-08-28T16:59:36+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>　　tensorflow针对训练、预测、服务端和移动端等环境支持多种模型格式，这对于初学者来说可能比较疑惑。目前，tf中主要包括.ckpt格式、.pb格式SavedModel和tflite四种格式的模型文件。SavedModel用于tensorflow serving环境中，tflite格式模型文件用在移动端，后续遇到相关格式模型文件会继续补充。这里主要介绍常见的ckpt和pb格式的模型文件，以及它们之间的转换方法。</p>
<h2 id="CheckPoint-ckpt"><a href="#CheckPoint-ckpt" class="headerlink" title="CheckPoint(*.ckpt)"></a>CheckPoint(*.ckpt)</h2><p>　　在使用tensorflow训练模型时，我们常常使用tf.train.Saver类保存和还原，使用该类保存和模型格式称为checkpoint格式。Saver类的save函数将图结构和变量值存在指定路径的三个文件中，restore方法从指定路径下恢复模型。当数据量和迭代次数很多时，训练常常需要数天才能完成，为了防止中间出现异常情况，checkpoint方式能帮助保存训练中间结果，避免重头开始训练的尴尬局面。有些地方说ckpt文件不包括图结构不能重建图是不对的，使用saver类可以保存模型中的全部信息。尽管ckpt模型格式对于训练时非常方便，但是对于预测却不是很好，主要有下面这几个缺点：</p>
<ol>
<li>ckpt格式的模型文件依赖于tensorflow，只能在该框架下使用;</li>
<li>ckpt模型文件保存了模型的全部信息，但是在使用模型预测时，有些信息可能是不需要的。模型预测时，只需要模型的结构和参数变量的取值，因为预测和训练不同，预测不需要变量初始化、反向传播或者模型保存等辅助节点;</li>
<li>ckpt将模型的变量值和计算图分开存储，变量值存在index和data文件中，计算图信息存储在meta文件中,这给模型存储会有一定的不方便。</li>
</ol>
<h2 id="frozen-model-pb"><a href="#frozen-model-pb" class="headerlink" title="frozen model(*.pb)"></a>frozen model(*.pb)</h2><p>　　Google推荐将模型保存为pb格式。PB文件本身就具有语言独立性，而且能被其它语言和深度学习框架读取和继续训练，所以PB文件是最佳的格式选择。另外相比ckpt格式的文件，pb格式可以去掉与预测无关的节点，单个模型文件也方便部署，因此实践中我们常常使用pb格式的模型文件。那么如何将ckpt格式的模型文件转化为pb的格式文件呢？主要包含下面几个步骤，结合这几个步骤写了个通用的脚本，使用该脚本只需指定ckpt模型路径、pb模型路径和模型的输出节点，多个输出节点时使用逗号隔开。</p>
<ul>
<li>通过传入的ckpt模型的路径得到模型的图和变量数据</li>
<li>通过 import_meta_graph 导入模型中的图</li>
<li>通过 saver.restore 从模型中恢复图中各个变量的数据</li>
<li>通过 graph_util.convert_variables_to_constants 将模型持久化</li>
<li>在frozen model的时候可以删除训练节点</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># -*-coding: utf-8 -*-</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.framework import graph_util</span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def freeze_graph(input_checkpoint,output_pb_path,output_nodes_name):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    :param input_checkpoint:</span><br><span class="line">    :param output_pb_path: PB模型保存路径</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    saver = tf.train.import_meta_graph(input_checkpoint + &#x27;.meta&#x27;, clear_devices=True)</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        saver.restore(sess, input_checkpoint) #恢复图并得到数据</span><br><span class="line">        graph = tf.get_default_graph()</span><br><span class="line">        # 模型持久化，将变量值固定</span><br><span class="line">        output_graph_def = graph_util.convert_variables_to_constants(  </span><br><span class="line">            sess=sess,</span><br><span class="line">            input_graph_def=sess.graph_def,</span><br><span class="line">            output_node_names=output_nodes_name.split(&quot;,&quot;))# 如果有多个输出节点，以逗号隔开</span><br><span class="line"></span><br><span class="line">        print(&quot;++++++++++++++%d ops in the freeze graph.&quot; % len(output_graph_def.node)) #得到当前图有几个操作节点</span><br><span class="line">        output_graph_def = graph_util.remove_training_nodes(output_graph_def)</span><br><span class="line">        print(&quot;++++++++++++++%d ops after remove training nodes.&quot; % len(output_graph_def.node)) #得到当前图有几个操作节点</span><br><span class="line"></span><br><span class="line">        # serialize and write pb model to Specified path</span><br><span class="line">        with tf.gfile.GFile(output_pb_path, &quot;wb&quot;) as f: </span><br><span class="line">            f.write(output_graph_def.SerializeToString()) </span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(&#x27;--ckpt_path&#x27;, type=str, required=True,help=&#x27;checkpoint file path&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--pb_path&#x27;, type=str, required=True,help=&#x27;pb model file path&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--output_nodes_name&#x27;, type=str, required=True,help=&#x27;name of output nodes separated by comma&#x27;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    freeze_graph(args.ckpt_path,args.pb_path,args.output_nodes_name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>参考：<br><a href="https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc">https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/tensorflow-model-channel-pruning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/tensorflow-model-channel-pruning/" class="post-title-link" itemprop="url">tensorflow模型通道剪枝(channel pruning)实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:39" itemprop="dateModified" datetime="2025-08-28T16:59:39+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>　　最近在做模型压缩(model compress)相关工作，之前分别尝试了权重量化(weight quantization)【1】和权重稀疏(weight sparsification)【2】，遗憾的是它们都需要推理引擎和硬件的特定优化才能实现推理加速，而tensorflow在x86架构的CPU下并没有没有针对量化和稀疏矩阵的优化，因此效果一般。吸取前两次的经验，这次尝试了结构化压缩通道剪枝(channel pruning)，它通过删减模型中冗余通道channel，减少的模型前向计算所需的FLOPs。通道剪枝来自论文ICCV2017论文 Channel Pruning for Accelerating Very Deep Neural Networks。 这里会首先简单介绍channel pruning的原理，然后通过PocketFlow压缩工具对ResNet56进行通道剪枝，结果显示channel pruning在精度不怎么损失的基础上，减小接近50%的FLOPs。由于剪枝后模型中增加了许多的conv2d 1x1卷积，实际提升推理效率大概20%。</p>
<h2 id="二、channel-pruning-基本原理"><a href="#二、channel-pruning-基本原理" class="headerlink" title="二、channel pruning 基本原理"></a>二、channel pruning 基本原理</h2><h3 id="1-什么是通道剪枝"><a href="#1-什么是通道剪枝" class="headerlink" title="1. 什么是通道剪枝"></a>1. 什么是通道剪枝</h3><p>　　虽然论文末尾谈到channel pruning可以应用到模型训练中，但是文章的核心内容还是对训练好的模型进行channel pruning，也就是文章中说的inference time。通道剪枝正如其名字channel pruning核心思想是移除一些冗余的channel简化模型。下图是从论文中截取的通道剪枝的示意图，它表示的网络模型中某一层的channel pruning。<strong>B</strong>表示输入feature map，<strong>C</strong>表示输出的feature map；c表示输入B的通道数量，n表示输出C的通道数量；<strong>W</strong>表示卷积核，卷积核的数量是n，每个卷积核的维度是c<em>kh</em>kw，kh和kw表示卷积核的size。通道剪枝的目的就是要把<strong>B</strong>中的某些通道剪掉，但是剪掉后的<strong>B</strong>和<strong>W</strong>的卷积结果能尽可能和<strong>C</strong>接近。当删减<strong>B</strong>中的某些通道时，同时也裁剪了<strong>W</strong>中与这些通道的对应的卷积核，因此通过通过剪枝能减小卷积的运算量。  </p>
<p><img src="/images/channel_pruning.jpg" alt="channel-pruning示意图"></p>
<h3 id="2-通道剪枝数学描述"><a href="#2-通道剪枝数学描述" class="headerlink" title="2. 通道剪枝数学描述"></a>2. 通道剪枝数学描述</h3><p>　　通道剪枝的思想是简单的，难点是怎么选择要裁剪的通道，同时要保证输出feature map误差尽可能得小，这也是文章的主要内容。channel pruning总体分为两个步骤，首先是channel selection，它是采用LASSO regression来做的，通过添加L1范数来约束权重，因为L1范数可以使得权重中大部分值为0，所以能使权重更加稀疏，这样就可以把那些稀疏的channel剪掉；第二步是reconstruction，这一步是基于linear least优化，使输出特征图变化尽可能的小。  </p>
<p>　　接下来通过数学表达式描述了通道剪枝。Ｘ($N*c* k_h*k_w$)表示输入feature map，W($n * c * k_h * k_w$)表示卷积核，Y($N*n$)表示输出feature map。$\beta_i$表示通道系数，如果等于0，表示该通道可以被删除。我们期望将输入feature map的channel从c压缩为c’($0&lt;&#x3D;c’&lt;&#x3D; c$)，同时要使得构造误差(reconstruction error)尽可能的小。通过下面的优化表达式，就可以选择哪些通道被删除。文章中详细介绍了怎么用算法解决下面的数据问题，这里就不赘述了。另外文章还考虑分支情况下的通道剪枝，例如ResNet和GoogleNet，感兴趣的可以仔细研读该论文【3】。</p>
<p><img src="/images/channel_pruning2.jpg" alt="channel-pruning示意图"></p>
<h2 id="三、PocketFlow"><a href="#三、PocketFlow" class="headerlink" title="三、PocketFlow"></a>三、PocketFlow</h2><p>　　PocketFlow是腾讯AI Lab开源的自动化深度学习模型压缩框架，它集成了腾讯自己研发的和来自其他同行的主流的模型压缩与训练算法，还引入了自研的超参数优化组件，实现了自动托管式模型压缩与加速。PocketFlow能够自动选择模型压缩的超参，极大的方便了算法人员的调参。这里主要使用里面的channel pruning算法（learner）进行通道剪枝。【4】</p>
<h3 id="1-实验准备"><a href="#1-实验准备" class="headerlink" title="1.实验准备:"></a>1.实验准备:</h3><p>1.cifar10数据集： <a href="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz">https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz</a><br>2.ResNet56预训练模型：<a href="https://share.weiyun.com/5610f11d61dfb733db1f2c77a9f34531">https://share.weiyun.com/5610f11d61dfb733db1f2c77a9f34531</a><br>3.下载Pocketflow: <a href="https://github.com/wxquare/PocketFlow.git">https://github.com/wxquare/PocketFlow.git</a></p>
<h3 id="2-准备配置文件path-conf"><a href="#2-准备配置文件path-conf" class="headerlink" title="2.准备配置文件path.conf"></a>2.准备配置文件path.conf</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># data files</span><br><span class="line">data_dir_local_cifar10 = ./cifar-10-binary/cifar-10-batches-bin #cifar10数据集解压的位置</span><br><span class="line"></span><br><span class="line"># model files </span><br><span class="line"># 这里模型文件用wget下载不下来，要登录下载，解压到PocketFlow根目录的model目录下面</span><br><span class="line">model_http_url = https://share.weiyun.com/5610f11d61dfb733db1f2c77a9f34531</span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<h3 id="3-在本地运行通道剪枝的learner"><a href="#3-在本地运行通道剪枝的learner" class="headerlink" title="3.在本地运行通道剪枝的learner"></a>3.在本地运行通道剪枝的learner</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \</span><br><span class="line">--learner=channel \</span><br><span class="line">--cp_uniform_preserve_ratio=0.5 \</span><br><span class="line">--cp_prune_option=uniform \</span><br><span class="line">--resnet_size=56</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="4-模型转换"><a href="#4-模型转换" class="headerlink" title="4. 模型转换"></a>4. 模型转换</h3><p>步骤3之后会在models产生ckpt文件，需要通过进行模型转化,最终会生成model_original.pb，model_transformed.pb，同时也会生成移动端对应的tflite文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python tools/conversion/export_chn_pruned_tflite_model.py \</span><br><span class="line">--model_dir=models/pruned_model </span><br><span class="line">--input_coll=train_images</span><br><span class="line">--output_coll=logits</span><br></pre></td></tr></table></figure>

<h2 id="四、剪枝前后模型分析"><a href="#四、剪枝前后模型分析" class="headerlink" title="四、剪枝前后模型分析"></a>四、剪枝前后模型分析</h2><p>　　我们可以通过之前介绍的模型基准测试工具benchmark_model分别测试剪枝前后的模型。可以很清楚看到通道剪枝大大减少了模型前向计算的FLOPs的变化，以及各阶段、算子的耗时和内存消耗情况。可以发现模型下降为原来的1&#x2F;2，卷积耗时下降接近50%。除此之外通过netron工具可以直观的看到模型通道剪枝前后结构发生的变化，通道剪枝之后的模型中明显增加了许多conv1*1的卷积。这里主要利用1x1卷积先降维，然后升维度，达到减少计算量的目的。1x1卷积还有多种用途，可以参考【5】。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ bazel-bin/tensorflow/tools/benchmark/benchmark_model \ </span><br><span class="line">--graph=model_original.pb \</span><br><span class="line">--input_layer=&quot;net_input&quot; \</span><br><span class="line">--input_layer_shape=&quot;1,32,32,3&quot; \</span><br><span class="line">--input_layer_type=&quot;float&quot; \</span><br><span class="line">--output_layer=&quot;net_output&quot; \</span><br><span class="line">--show_flops=true \</span><br><span class="line">--show_run_order=false \</span><br><span class="line">--show_time=true \</span><br><span class="line">--num_threads=1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/images/channel_pruning3.jpg" alt="channel-pruning 1x1 convolution"></p>
<p>参考：<br>[1]. <a href="https://wxquare.github.io/2019/09/16/other/tensorflow-model-quantization/">tensorflow模型权重量化(weight quantization)实战</a><br>[2]. <a href="https://wxquare.github.io/2019/09/27/other/tensorflow-model-no-structural-pruning">tensorflow模型权重稀疏(weight sparsification)实战</a><br>[3].<a href="https://arxiv.org/abs/1707.06168">Channel Pruning for Accelerating Very Deep Neural Networks</a><br>[4].<a href="https://github.com/wxquare/PocketFlow">PocketFLow</a><br>[5].1x1卷积：<a href="https://www.zhihu.com/question/56024942">https://www.zhihu.com/question/56024942</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/tensorflow-model-no-structural-pruning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/tensorflow-model-no-structural-pruning/" class="post-title-link" itemprop="url">tensorflow模型权重稀疏(weight sparsification)实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:42" itemprop="dateModified" datetime="2025-08-28T16:59:42+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>　　深度模型通常会有更好的预测精度，但是它面临计算开销过大的问题。模型压缩(model compress)是提高深度模型推理效率的一种解决方案，它期望在不损失精度或者精度损失可控的范围内，加速推理效率，减低内存开销。目前，模型压缩算法主要包括权<strong>重量化(quantization)、剪枝(pruning)、低秩分解等</strong>。上周尝试了<a href="https://wxquare.github.io/2019/09/16/other/tensorflow-model-quantization/">tensorflow中的模型量化</a>，发现量化需要硬件或者推理引擎的对低精度8-bit计算支持，目前tensorflow在x86和gpu环境下还没有很好的支持，因此量化只帮助实现了模型大小下降，没有实现推理的加速。model pruning学习的材料是tensorflow repo中的tensorflow&#x2F;contrib&#x2F;model_pruning，实际了解后发现它属于pruning中no-structural pruning，其加速效果依赖具体的硬件实现，加速效果一般，tensorflow 中对稀疏矩阵运算没有特别好的优化（依赖于底层的 SparseBLAS 实现，目前还没有特别好的）。model pruning中还有一种structural pruning 则不改变计算方式，可以直接使用，加速效果相对较好，之后也会继续尝试。</p>
<h2 id="二、tensorflow-contrib-model-pruning原理"><a href="#二、tensorflow-contrib-model-pruning原理" class="headerlink" title="二、tensorflow&#x2F;contrib&#x2F;model_pruning原理"></a>二、tensorflow&#x2F;contrib&#x2F;model_pruning原理</h2><p>　　<a href="https://arxiv.org/pdf/1710.01878.pdf">Michael Zhu and Suyog Gupta, “To prune, or not to prune: exploring the efficacy of pruning for model compression”, 2017 NIPS </a><br>　　tensorflow中model_pruning理论来自上面这篇文章。文章中指出目前有些深度学习网络模型是过度设计（over-parameterized）。为了使其在资源受限的环境下高效的进行推理预测，要么减少网络的隐藏单元（hidden unit）同时保持模型密集连接结构，要么采用针对大模型进行模型剪枝（model pruning）。文章中的模型行剪枝是一种非结构化的剪枝（no-structural pruning），它在深度神经网络的各种连接矩阵中引入稀疏性（sparsity），从而减少模型中非零值参数的数量。文章比较了大而稀疏（large-sparse）和较小密集（small-dense）这两种模型，认为前者是优于后者的。除此之外，文章提出了一种新的渐进剪枝技术（gradual pruning technique），它能比较方便的融入到模型训练的过程中，使其调整比较小。</p>
<p>　　tensorflow中的模型剪枝是一种训练时剪枝。对于需要被剪枝的网络模型，对于网络中每个需要被剪枝的层（layer)添加一个二进制掩码变量（binary mask variable ），该变量的大小和形状和改层的权重张量（weight tensor）相同。在训练图中加入一些ops，它负责对该层的权重值（weights）的绝对值进行排序，通过mask将最小的权重值屏蔽为0。在前向传播时该掩模的对应位与选中权重进行相与输出feature map，如果该掩模对应位为0则对应的权重相与后则为0，在反向传播时掩模对应位为0的权重参数则不参与更新。除此之外，文章提出了一种新的自动逐步修剪算法（automated gradual pruning），它实际上是定义了一种稀疏度变化的规则，初始时刻，稀疏度提升较快，而越到后面，稀疏度提升速度会逐渐放缓，这个主要是基于冗余度的考虑。因为初始时有大量冗余的权值，而越到后面保留的权值数量越少，不能再“大刀阔斧”地修剪，而需要更谨慎些，避免“误伤无辜”。其表达式如下，官方文档中列出了一些的剪枝超参数，主要的有下面几个。<br>$$s_{t}&#x3D;s_{f}+\left(s_{i}-s_{f}\right)\left(1-\frac{t-t_{0}}{n\Delta t}\right)^{3}  $$</p>
<ul>
<li>initial_sparsity：初始稀疏值$s_i$</li>
<li>target_sparsity：目标稀疏值$s_f$</li>
<li>sparsity_function_begin_step：开始剪枝的step $t_0$</li>
<li>sparsity_function_end_step: 剪枝停止的step</li>
<li>pruning_frequency：剪枝的频率$\Delta t$，文章提出在100到1000之间通常比较好</li>
<li>sparsity_function_exponent: 剪枝函数的指数，表示式中已描述为默认的3，表示由快到慢，为1时表示线性剪枝</li>
</ul>
<h2 id="三、tensorflow中的model-pruning实践"><a href="#三、tensorflow中的model-pruning实践" class="headerlink" title="三、tensorflow中的model_pruning实践"></a>三、tensorflow中的model_pruning实践</h2><p>　　tensorflow中model_pruning的源码位于tensorflow&#x2F;contrib&#x2F;model_pruning。</p>
<ol>
<li><p>准备tensorflow-1.14.0源码</p>
</li>
<li><p>编译model_pruning</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$bazel build -c opt tensorflow/contrib/model_pruning/examples/cifar10:cifar10_train</span><br></pre></td></tr></table></figure></li>
<li><p>通过设置一些参数，开始针对cifar10剪枝</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$bazel-out/k8-py2-opt/bin/tensorflow/contrib/model_pruning/examples/cifar10/cifar10_train \</span><br><span class="line">--train_dir=/home/terse/code/programming/tensorflow/model_pruning/train \</span><br><span class="line">--pruning_hparams=name=cifar10_pruning,\</span><br><span class="line">initial_sparsity=0.3,\</span><br><span class="line">target_sparsity=0.9,\</span><br><span class="line">sparsity_function_begin_step=100,\</span><br><span class="line">sparsity_function_end_step=10000</span><br></pre></td></tr></table></figure>
</li>
<li><p>可通过tensorboard查看剪枝过程。可以清楚的看出随着训练步骤的增加，conv1和conv2的sparsity在不断的增长。 在GRAPHS 页面，双击conv节点，可以看到在原有计算图基础上新增了mask和threshold节点用来做 model pruning</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$tensorboard --logdir=/home/terse/code/programming/tensorflow/model_pruning/train</span><br></pre></td></tr></table></figure>
</li>
<li><p>模型剪枝之后将剪枝的ops从训练图中删除。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$bazel build -c opt tensorflow/contrib/model_pruning:strip_pruning_vars</span><br><span class="line">$bazel-out/k8-py2-opt/bin/tensorflow/contrib/model_pruning/strip_pruning_vars \</span><br><span class="line">--checkpoint_dir=/home/terse/code/programming/tensorflow/model_pruning/train \</span><br><span class="line">--output_node_names=softmax_linear/softmax_linear_2 \</span><br><span class="line">--output_dir=/home/terse/code/programming/tensorflow/model_pruning \</span><br><span class="line">--filename=pruning_stripped.pb</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="四、model-pruning源码简单分析"><a href="#四、model-pruning源码简单分析" class="headerlink" title="四、model_pruning源码简单分析"></a>四、model_pruning源码简单分析</h2><p>　　使用tensorflow的model_pruning进行模型剪枝，主要包括两方面的工作，一是apply_mask，二是在训练图中增加剪枝的节点（pruning ops）。这里分别截取了其中的两段代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># cifar10_pruning.py  apply_mask to the graph</span><br><span class="line">with tf.variable_scope(&#x27;conv1&#x27;) as scope:</span><br><span class="line">  kernel = _variable_with_weight_decay(</span><br><span class="line">      &#x27;weights&#x27;, shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0)</span><br><span class="line"></span><br><span class="line">  conv = tf.nn.conv2d(</span><br><span class="line">      images, pruning.apply_mask(kernel, scope), [1, 1, 1, 1], padding=&#x27;SAME&#x27;)</span><br><span class="line">  </span><br><span class="line">  biases = _variable_on_cpu(&#x27;biases&#x27;, [64], tf.constant_initializer(0.0))</span><br><span class="line">  pre_activation = tf.nn.bias_add(conv, biases)</span><br><span class="line">  conv1 = tf.nn.relu(pre_activation, name=scope.name)</span><br><span class="line">  _activation_summary(conv1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"> #Adding pruning ops to the training graph</span><br><span class="line">with tf.graph.as_default():</span><br><span class="line"></span><br><span class="line">  # Create global step variable</span><br><span class="line">  global_step = tf.train.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">  # Parse pruning hyperparameters</span><br><span class="line">  pruning_hparams = pruning.get_pruning_hparams().parse(FLAGS.pruning_hparams)</span><br><span class="line"></span><br><span class="line">  # Create a pruning object using the pruning specification</span><br><span class="line">  p = pruning.Pruning(pruning_hparams, global_step=global_step)</span><br><span class="line"></span><br><span class="line">  # Add conditional mask update op. Executing this op will update all</span><br><span class="line">  # the masks in the graph if the current global step is in the range</span><br><span class="line">  # [begin_pruning_step, end_pruning_step] as specified by the pruning spec</span><br><span class="line">  mask_update_op = p.conditional_mask_update_op()</span><br><span class="line"></span><br><span class="line">  # Add summaries to keep track of the sparsity in different layers during training</span><br><span class="line">  p.add_pruning_summaries()</span><br><span class="line"></span><br><span class="line">  with tf.train.MonitoredTrainingSession(...) as mon_sess:</span><br><span class="line">    # Run the usual training op in the tf session</span><br><span class="line">    mon_sess.run(train_op)</span><br><span class="line"></span><br><span class="line">    # Update the masks by running the mask_update_op</span><br><span class="line">    mon_sess.run(mask_update_op)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="五、总结和未解决的问题"><a href="#五、总结和未解决的问题" class="headerlink" title="五、总结和未解决的问题"></a>五、总结和未解决的问题</h2><ol>
<li>tensorflow中的模型剪枝属于no-structral，本质上是使权重稀疏化(weight sparsification),实践中发现它没有使推理加速，据其加速效果依赖具体的硬件实现，加速效果一般，tensorflow 中对稀疏矩阵运算没有特别好的优化（依赖于底层的 SparseBLAS 实现，目前还没有特别好的）</li>
<li>实践中发现不管稀疏度为多少，其剪枝后的模型大小都是相同的，是不是tensorflow对稀疏的模型也是按照非稀疏格式存储的？</li>
<li>issue:<a href="https://github.com/tensorflow/tensorflow/issues/32805">model_pruning: Why 50% and 90% zeros of the stripped models are the same size? #32805</a></li>
<li>issue: [CNN.Model pruning: no gain in speeding up of inference #22732](CNN.Model pruning: no gain in speeding up of inference #22732)</li>
</ol>
<p>参考：</p>
<ol>
<li><a href="https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/contrib/model_pruning">https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/contrib/model_pruning</a></li>
<li><a href="https://arxiv.org/pdf/1710.01878.pdf">Michael Zhu and Suyog Gupta, “To prune, or not to prune: exploring the efficacy of pruning for model compression”, 2017 NIPS </a></li>
<li><a href="https://zhuanlan.zhihu.com/p/48069799">https://zhuanlan.zhihu.com/p/48069799</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/tensorflow-model-quantization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/tensorflow-model-quantization/" class="post-title-link" itemprop="url">tensorflow模型权重量化(weight quantization)实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:46" itemprop="dateModified" datetime="2025-08-28T16:59:46+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>　　<br>　　最近在尝试深度学习模型加速的工作，查了一些资料，发现模型推理加速的研究还挺多的，主要从四个方面进行，从头开始构建轻量高效的模型，例如mobileNets、squeezenet等；通过量化(quantization)、裁剪(pruning)和压缩(compression)来降低模型的尺寸；通过高效的计算平台加速推理(inference)的效率，例如Nvidia TensorRT、GEMMLOWP、Intel MKL-DNN等以及硬件定制。考虑到自身的能力，遵循从简单到复杂、通用到专用的原则，选择从模型量化(model quantization)入手，之后会陆续尝试其他优化手段。在一番尝试之后，挺遗憾的，因为tensorflow模型量化并没有使模型预测(inference)加速，根据tf成员在issue的回复，tf的模型量化主要针对移动端的优化，目前还没有针对x86和gpu环境的优化。<strong>有成功通过模型量化加速推理过程的同学欢迎打脸留言</strong>。</p>
<h2 id="一、为什么要模型量化"><a href="#一、为什么要模型量化" class="headerlink" title="一、为什么要模型量化"></a>一、为什么要模型量化</h2><p>　　为了尽可能保证深度学习模型的准确度(precision)，在训练和推理时候通常使用float32格式的数据。然而在实际商用中，有些模型由于层数和参数都比较多，推理预测需要很大计算量，导致推理(inference)的效率很低。模型量化(model quantization)是通用的深度学习优化的手段之一，它通过将float32格式的数据转变为int8格式，一方面降低内存和存储的开销，同时在一定的条件下(8-bit低精度运算 low-precision)也能提升预测的效率。目前还不太理解8-bit低精度运算，猜测这是模型量化没有实现推理加速的原因。模型量化适用于绝大数模型和使用场景，对于训练后的量化，不需要重新训练模型，可以很快将其量化为定点模型，而且几乎不会有精度损失，因此模型量化追求更小的模型和更快的推理速度。<strong>实验中量化确实时模型下降为原来的1&#x2F;4，但在推理效率并没有提升，甚至略有下降</strong>。</p>
<h2 id="二、什么是量化"><a href="#二、什么是量化" class="headerlink" title="二、什么是量化"></a>二、什么是量化</h2><h3 id="2-1-实数量化"><a href="#2-1-实数量化" class="headerlink" title="2.1 实数量化"></a>2.1 实数量化</h3><p>　　网络上关于模型量化的内容挺多的，量化本质上是一种仿射图(affine map)，它以表达式(1)将实数值表示映射为量化的uint8，当然也可以等效为表示式(2): </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">real_value = A * quantized_value + B             (1) </span><br><span class="line">real_value = C * (quantized_value + D)           (2) </span><br></pre></td></tr></table></figure>

<p>　　除此之外，深度学习模型量化中有一个<strong>约束条件，0必须准确的表示，不能有误差</strong>。因为对于某些神经网络层，实数0精确表示对于优化实现非常有用，例如在具有填充的卷积层或池化层中，长度对输入数组进行零填充(zero-padding)来实现填充是有用的。实数值0对应的量化值称之为零点(zero-point)。实际上，如果0不能完全表示，当我们用0对应的量化值进行填充时，因为这与实际值0不完全对应，会导致结果不准确，引入偏差。因此有：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">　　0=A∗zero_point+B</span><br><span class="line">　　zero_point=−B/A</span><br><span class="line">　　0=C∗(zero_point+D)</span><br><span class="line">　　0=zero_point+D</span><br><span class="line">　　D=−zero_point</span><br></pre></td></tr></table></figure>



<p>　　结合上述条件，可以得出量化的最终表达式为(3)，它能做到0值的准确表示，zero_point是0对应的量化值。表示式(3)中有两个常量，zero_point是量化值，通常是uint8值，scale是一个正实数，通常为float32。<br>$$real\_value &#x3D; scale * (quantized\_value - zero\_point)　　(3)$$</p>
<h3 id="2-2-矩阵乘法量化"><a href="#2-2-矩阵乘法量化" class="headerlink" title="2.2 矩阵乘法量化"></a>2.2 矩阵乘法量化</h3><p>　　根据表达式(3)，我们可以将实数值(通常为float32)用量化值(通常为uint8)表示，下面将介绍怎么把它应用到矩阵乘法当中。假设有两个实数矩阵$lhs\_real\_matrix, rhs\_real\_matrix$，量化之后就会有对应的$lhs\_scale, rhs\_scale, lhs\_zero\_point, rhs\_zero\_point$，矩阵中的实数值可以用其量化值表示为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lhs_real_value[i] = lhs_scale * (lhs_quantized_value[i] - lhs_zero_point)</span><br><span class="line">rhs_real_value[i] = rhs_scale * (rhs_quantized_value[i] - rhs_zero_point)</span><br></pre></td></tr></table></figure>
<p>　　在矩阵乘法中，每个值($result\_real\_value$)都由对应的ｉ个值相乘累加得到，根据表达式(4)和(5)很容易得到表示式(6),它表示$result\_quantized\_value$可由$lhs\_quantized\_value、rhs\_quantized\_value$计算得出。注意这里面有几个问题需要解决，如何减小式(6)中与zero_point减法的开销(overhead)？如何将(lhs_scale * rhs_scale &#x2F; result_scale)实数运算用整数运算处理？这部分的内容参考gemmlowp的实现。<br>　　<a href="https://github.com/google/gemmlowp/blob/master/doc/quantization.md">https://github.com/google/gemmlowp/blob/master/doc/quantization.md</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">result_real_value</span><br><span class="line">  = Sum_over_i(lhs_real_value[i] * rhs_real_value[i])</span><br><span class="line">  = Sum_over_i(</span><br><span class="line">        lhs_scale * (lhs_quantized_value[i] - lhs_zero_point) *</span><br><span class="line">        rhs_scale * (rhs_quantized_value[i] - rhs_zero_point)</span><br><span class="line">    )</span><br><span class="line">  = lhs_scale * rhs_scale * Sum_over_i(</span><br><span class="line">        (lhs_quantized_value[i] - lhs_zero_point) *</span><br><span class="line">        (rhs_quantized_value[i] - rhs_zero_point)</span><br><span class="line">    )                    (4)</span><br><span class="line"></span><br><span class="line">result_real_value = result_scale * (result_quantized_value - result_zero_point)</span><br><span class="line">result_quantized_value = result_zero_point + result_real_value / result_scale  (5)</span><br><span class="line"></span><br><span class="line">result_quantized_value = result_zero_point +</span><br><span class="line">    (lhs_scale * rhs_scale / result_scale) *</span><br><span class="line">        Sum_over_i(</span><br><span class="line">            (lhs_quantized_value[i] - lhs_zero_point) *</span><br><span class="line">            (rhs_quantized_value[i] - rhs_zero_point)</span><br><span class="line">        )          (6)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="三、tensorflow模型量化方案"><a href="#三、tensorflow模型量化方案" class="headerlink" title="三、tensorflow模型量化方案"></a>三、tensorflow模型量化方案</h2><p>　　**训练后量化(post training Quantization)**。在许多情况下，我们希望在不重新训练模型的前提下，只是通过压缩权重或量化权重和激活输出来缩减模型大小，从而加快预测速度。“训练后量化”就是这种使用简单，而且在有限的数据条件下就可以完成量化的技术。训练后量化操作简单，只需要使用量化工具将训练好的模型执行量化类型，即可实现模型的量化。训练后量化包括“只对权重量化”和“对权重和激活输出都量化”，对于很多网络而言，都可以产生和浮点型很接近的精度。</p>
<p>　　**只对权重量化(weight only quantization)**。一个简单的方法是只将权重的精度从浮点型减低为8bit整型。由于只有权重进行量化，所以无需验证数据集就可以实现。一个简单的命令行工具就可以将权重从浮点型转换为8bit整型。如果只是想为了方便传输和存储而减小模型大小，而不考虑在预测时浮点型计算的性能开销的话，这种量化方法是很有用的。</p>
<p>　　<strong>量化权重和激活输出（Quantizing weights and activations）</strong>。我们可以通过计算所有将要被量化的数据的量化参数，来将一个浮点型模型量化为一个8bit精度的整型模型。由于激活输出需要量化，这时我们就得需要标定数据了，并且需要计算激活输出的动态范围，一般使用100个小批量数据就足够估算出激活输出的动态范围了。</p>
<p>　　**训练时量化（Quantization Aware Training)**。训练时量化方法相比于训练后量化，能够得到更高的精度。训练时量化方案可以利用Tensorflow的量化库，在训练和预测时在模型图中自动插入模拟量化操作来实现。由于训练时量化相对麻烦，加上权重量化没有实现加速的期望，所以没有尝试训练时量化，根据文档显示，其大概包括以下几个步骤：</p>
<ol>
<li>可以在预训练好的模型基础上继续训练或者重新训练，建议在保存好的浮点型模型的基础上精调</li>
<li>修改估计器，添加量化运算，利用tf.contrib.quantize中的量化rewriter向模型中添加假的量化运算</li>
<li>训练模型，输出对于权重和激活输出都带有各自量化信息（尺度、零点）的模型</li>
<li>转换模型，利用tf.contrib.lite.toco convert定义的转换器，将带有量化参数的模型被转化成flatbuffer文件，该文件会将权重转换成int整型，同时包含了激活输出用于量化计算的信息</li>
<li>执行模型，转换后的带有整型权重的模型可以利用TFLite interpreter来执行，也可以在CPU上运行模型</li>
</ol>
<h2 id="四、tensorflow模型权重量化实验"><a href="#四、tensorflow模型权重量化实验" class="headerlink" title="四、tensorflow模型权重量化实验"></a>四、tensorflow模型权重量化实验</h2><p>　　一开始尝试模型量化是因为有个复杂的视频分割模型推理效率很低，期望通过模型量化实现加速，在复杂模型上尝试失败之后，我用label_image的例子再次验证，结果显示也没有加速的效果。这里主要试验了训练后量化，尝试了只对权重量化和权重和激活量化，发现后者比前者性能更差，这里描述权重量化的过程。整个过程是比较简单的，tensorflow有两种量化方式，推荐使用第二种，编译命令行工具进行量化。</p>
<ol>
<li><p>在tensorflow r1.0的版本中有个量化的脚本可以提供量化的功能：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$wget &quot;https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz&quot;</span><br><span class="line">$tar -xzf tensorflow/examples/label_image/data</span><br><span class="line">$ work_dir=/home/terse/code/programming/tensorflow/quantization</span><br><span class="line">$ python tensorflow/tools/quantization/quantize_graph.py \</span><br><span class="line">--input=$work_dir/inception_v3_2016_08_28_frozen.pb \</span><br><span class="line">--output=$work_dir/inception_quantized0.pb \</span><br><span class="line">--output_node_names=InceptionV3/Predictions/Reshape_1 \</span><br><span class="line">--mode=weights </span><br></pre></td></tr></table></figure>
</li>
<li><p>在较新版本的tf中，quantize_graph.py量化的脚本已经废弃了需要编译tensorflow的源码生成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensorflow-1.14.0编译transform_graph工具</span><br><span class="line">$ bazel build tensorflow/tools/graph_transforms:transform_graph</span><br><span class="line">$ bazel-bin/tensorflow/tools/graph_transforms/transform_graph \</span><br><span class="line">--in_graph=$work_dir/inception_v3_2016_08_28_frozen.pb \</span><br><span class="line">--out_graph=$work_dir/inception_quantized1.pb \</span><br><span class="line">--outputs=InceptionV3/Predictions/Reshape_1 \</span><br><span class="line">--transforms=&#x27;quantize_weights&#x27;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用summarize_graph分析量化前后的模型区别，权重量化、模型减小、增加了一些和量化和反量化的节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensorflow-1.14.0编译transform_graph工具</span><br><span class="line">$ bazel build tensorflow/tools/graph_transforms:summarize_graph</span><br><span class="line">$ bazel-bin/tensorflow/tools/graph_transforms/summarize_graph \</span><br><span class="line">--in_graph=$work_dir/inception_quantized1.pb \</span><br><span class="line">--print_structure=true</span><br></pre></td></tr></table></figure></li>
<li><p>使用权重量化的模型做推理验证</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ bazel build tensorflow/examples/label_image：label_image</span><br><span class="line">$ bazel-bin/tensorflow/examples/label_image/label_image \</span><br><span class="line">--image=$work_dir/grace_hopper.jpg \</span><br><span class="line">--labels=$work_dir/imagenet_slim_labels.txt \</span><br><span class="line">--graph=$work_dir/inception_quantized1.pb</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="五、-为什么模型量化没有使推理加速"><a href="#五、-为什么模型量化没有使推理加速" class="headerlink" title="五、 为什么模型量化没有使推理加速"></a>五、 为什么模型量化没有使推理加速</h2><p>　　关于tensorflow模型量化没有实现模型加速的，我查了一些资料，发现出现类似的问题不在少数。根据tensorflow团队成员的回复，截了几个member的答复，大意是目前量化目前针对移动端的优化，当然也有一些移动端的人说速度下降了。tensorflow未来有可能针对intel x86，gpu量化优化，但不知道什么时候支持。</p>
<p>　　The quantization is aimed at mobile performance, so most of the optimizations are for ARM not x86. We’re hoping to get good quantization on Intel eventually, but we don’t have anyone actively working on it yet.</p>
<p>　　Quantized ops currently only work on the CPU, because most GPUs don’t support eight-bit matrix multiplications natively. I have just seen that the latest TitanX Pascal cards offer eight-bit support though, so I’m hoping we will be able to use that in the future.</p>
<p>参考：</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/33535898">https://zhuanlan.zhihu.com/p/33535898</a></li>
<li><a href="https://arxiv.org/abs/1806.08342">https://arxiv.org/abs/1806.08342</a></li>
<li><a href="https://github.com/google/gemmlowp/blob/master/doc/quantization.md">https://github.com/google/gemmlowp/blob/master/doc/quantization.md</a></li>
<li><a href="https://github.com/tensorflow/tensorflow/issues/2807">https://github.com/tensorflow/tensorflow/issues/2807</a></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/visp-template-tracker/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/visp-template-tracker/" class="post-title-link" itemprop="url">了解模板追踪算法和高斯牛顿迭代法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 17:00:07" itemprop="dateModified" datetime="2025-08-28T17:00:07+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>　　最近在项目中使用到visp库的模板追踪算法(template tracker)，由于接触算法的时间比较短，这里简单记录对算法的理解和认识。模板追踪算法原理比较简单，当代价函数为SSD时，抽象为数学中的非线性最优化问题，这里采用高斯牛顿法求解。高斯牛顿法应该是通用的一种求最优化问题的算法，高斯牛顿法核心是迭代公式，不断迭代更新出新的参数值。visp模板算法效率本身不高，因此在实现的时候提供了一些可调的优化的参数，例如金字塔、采样率、迭代次数、误差等。在项目中，visp模板追踪算法在参考模板没有遮挡的情况下，效果基本满足要求，但是在有遮挡的情况，会存在比较大的问题，因此我们针对遮挡情况，进行了特别的优化。除此之外，我们优化了一个并行版本的模板追踪算法，提升追踪效率。</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>　　在了解visp模板追踪算法之前，可通过官网上的<a href="https://visp.inria.fr/template-tracking/">视频</a>了解追踪算法的能力。它和kcf之类的追踪算法还不太一样，在kcf追踪算法中，我们需要告诉追踪器的追踪目标，通常情况下，我们不要求像素级别的进度的要求。而template tracker参考模板（reference template）计算视频中两帧之间的单应矩阵Homography，通过单应矩阵计算目标区域在当前帧的位置，从而实现追踪的效果。</p>
<h2 id="数学描述"><a href="#数学描述" class="headerlink" title="数学描述"></a>数学描述</h2><p>　　visp库中为模板追踪算法提供了SSD、ZNNC和在VISP 3.0.0时引入的MI(mutual information) 代价函数。这里以SSD代价函数描述模板追踪算法。模板追踪算法在数学描述为一个最优化问题，通过SSD代价函数，缩小误差，寻找最优的标记帧到当前帧的单应矩阵。模板追踪算法的数学描述如下：<br>$$ H_t &#x3D; \arg \min \limits_{H}\sum_{x∈ROI}((I^*(x)-I_t(w(x,H)))^2 $$</p>
<ul>
<li>$I^*$表示标记帧(参考帧），$I_t$表示当前帧</li>
<li>ROI表示参考区域（参考模板，reference template)</li>
<li>$H$ 表示参考帧$I^*$到当前帧的的单应矩阵Homography</li>
<li>$x$ 表示图像中的一个像素点</li>
<li>$w(x,H)$ 表示标记帧上像素点$x$根据单应矩阵$H$到当前帧的映射关系</li>
</ul>
<p>　　这里使用经典的<strong>高斯牛顿法(Gauss–Newton algorithm)迭代法</strong>求解，关于高斯牛顿法这里就不赘述了，最关键的是其迭代公式，感兴趣可以参考下面两篇文章：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm">https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/42383070">https://zhuanlan.zhihu.com/p/42383070</a></li>
</ul>
<p>　　其迭代公式如下，$J$表示雅克比矩阵，$J^T$表示$J$的转置，$H_t$表示迭代的初始值，$H_k$表示上一次迭代的结果，$r(H_k)$表示上一次迭代的残差residual。</p>
<p>$$ H_{t+1} &#x3D; H_t + (J^TJ)^{-1}J^Tr(H_k)  $$</p>
<h2 id="关键实现步骤"><a href="#关键实现步骤" class="headerlink" title="关键实现步骤"></a>关键实现步骤</h2><p>　　了解了模板追踪算法的数学描述和高斯牛顿迭代算法，其基本实现应该是不难的，它本质上是一个迭代算法主要分为以下几步：<br>step1. 设定初始的$H$矩阵，第一帧为单一矩阵，之后上一帧的结果.<br>step2. 对于第$k$次迭代计算雅克比$J$, 残差$r(H_k)$，得到$\triangledown H&#x3D;-(J^TJ)^{-1}J^Tr(H_k)$.<br>step3. 如果$\triangledown H$ 足够小或者达到最大循环次数就停止迭代<br>step4. 如果不满足迭代停止条件$H_{k+1}&#x3D;H_{k} +\triangledown H$<br>step5. 迭代结束时，$H_{t+1}&#x3D;H_{k}$</p>
<h3 id="1-计算关键帧中的参考区域中-reference-template）中每个像素点的雅克比"><a href="#1-计算关键帧中的参考区域中-reference-template）中每个像素点的雅克比" class="headerlink" title="1.计算关键帧中的参考区域中(reference template）中每个像素点的雅克比:"></a>1.计算关键帧中的参考区域中(reference template）中每个像素点的雅克比:</h3><ul>
<li>计算关于x方向的梯度</li>
<li>计算关于y方向的梯度</li>
<li>对ROI中的每个点uv计算$J&#x3D;[d_xu,d_xv,d_x,d_yu,d_yv,d_y,-d_xu^2-d_yuv,-d_xuv-d_yv^2]$<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># img0 表示标记帧</span><br><span class="line">   dx = cv2.Sobel(img0, cv2.CV_64F, 1, 0, ksize)</span><br><span class="line">   dy = cv2.Sobel(img0, cv2.CV_64F, 0, 1, ksize)</span><br><span class="line">   img0 = cv2.GaussianBlur(img0, (ksize, ksize), 1)</span><br><span class="line"></span><br><span class="line"># uv表示标记帧参考区域的每个像素点</span><br><span class="line">   juv = [dx[uv] * u, dx[uv] * v, dx[uv], dy[uv] * u, dy[uv] * v, dx[uv],</span><br><span class="line">          -dx[uv] * u * u - dy[uv] * u * v, -dx[uv] * u * v - dy[uv] * v * v]</span><br><span class="line">J = np.array(juv).T</span><br><span class="line"></span><br><span class="line"># MJ=-(JT*J)^-1 *JT</span><br><span class="line">   MJ = -np.dot(np.linalg.pinv(np.dot(J.T, J)), J.T)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-迭代计算当前帧的H的矩阵"><a href="#2-迭代计算当前帧的H的矩阵" class="headerlink" title="2.迭代计算当前帧的H的矩阵"></a>2.迭代计算当前帧的H的矩阵</h3><ul>
<li>迭代条件停止的条件，两次迭代误差小于一个指定值，例如$10^{-8}$</li>
<li>第一次为单位矩阵，之后为上一帧的追踪结果</li>
<li>根据H矩阵将关键帧上上参考区域的点映射到当前帧: uv1 &#x3D; np.dot(H, uv)</li>
<li>计算关键帧上参考区域到当前帧的误差e：E &#x3D; img0[uv] - img1[uv1] </li>
<li>计算$\triangledown H &#x3D; -(J^TJ)^{-1}J_ne_n$</li>
<li>计算新的$H$<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># for deltaH</span><br><span class="line">MJ = -np.dot(np.linalg.pinv(np.dot(J.T, J)*lambdaJTJ), J2.T)</span><br><span class="line">#MJ = -np.dot(np.linalg.pinv(np.dot(J2.T, J2)*lambdaJTJ), J2.T)</span><br><span class="line">deltaH =alpha* np.dot(MJ, E2)</span><br><span class="line"></span><br><span class="line"># for newH</span><br><span class="line">dh = np.insert(deltaH, 8, np.zeros(1), 0).reshape((3, 3))</span><br><span class="line">dhinv = np.linalg.pinv(np.identity(3) + dh)</span><br><span class="line">newH = np.dot(H, dhinv)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="实际实现考虑点及其存在的问题"><a href="#实际实现考虑点及其存在的问题" class="headerlink" title="实际实现考虑点及其存在的问题"></a>实际实现考虑点及其存在的问题</h2><p>为提高模板追踪算法的效率，visp库在实现模板追踪算法的时候设置了一些可调的参数：</p>
<ul>
<li>对参考模板中的像素点进行采样处理setSampling</li>
<li>迭代时设置学习率，setLambda默认为0.001</li>
<li>设置最大迭代次数，setIterationMax(200)</li>
<li>设置金字塔的层数，tracker.setPyramidal(2, 1)</li>
</ul>
<p>　　实际使用visp模板追踪算法中，发现当参考模板处有物体遮挡时，效果不好，因此需要做进一步的处理。另外，我们在工程实践时，为了提高追踪的效率，升级了一个并行版本的追踪，能提高数倍的追踪效率。</p>
<p>参考链接：</p>
<ul>
<li><a href="https://visp.inria.fr/template-tracking/">https://visp.inria.fr/template-tracking/</a></li>
<li><a href="https://visp-doc.inria.fr/doxygen/visp-daily/tutorial-tracking-tt.html">https://visp-doc.inria.fr/doxygen/visp-daily/tutorial-tracking-tt.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm">https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/42383070">https://zhuanlan.zhihu.com/p/42383070</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/13/AI/%E5%88%9D%E5%A7%8BOpenCL%E5%8F%8A%E5%9C%A8%E7%9A%84%E7%A7%BB%E5%8A%A8%E7%AB%AF%E7%9A%84%E4%B8%80%E4%BA%9B%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="wxquare">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | wxquare's Blogs">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/13/AI/%E5%88%9D%E5%A7%8BOpenCL%E5%8F%8A%E5%9C%A8%E7%9A%84%E7%A7%BB%E5%8A%A8%E7%AB%AF%E7%9A%84%E4%B8%80%E4%BA%9B%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE/" class="post-title-link" itemprop="url">初识OpenCL及在移动端的一些测试数据</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-13T00:00:00+08:00">2020-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-08-28 16:59:18" itemprop="dateModified" datetime="2025-08-28T16:59:18+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>　　最近在做kcf算法在移动端优化的相关工作，由于kcf算法计算量太大，而移动端计算性能有限，因此打算将kcf部分耗时操作通过GPU计算进行提升算法的性能。由于接触GPU和OpenCL的时间比较短，原理性的东西理解得也不深刻，本文主要在移动端测试了一些GPU和OpenCL的数据，无法分析内在原因，方便后续移动端算法优化。主要工作如下：</p>
<ol>
<li>编译了OpenCL的opencv版本sdk，测试了mat到umat相互内存拷贝和cvtcolor函数的性能。</li>
<li>测试了OpenCL核心API的性能</li>
<li>以内存拷贝核函数为例，测试OpenCL work_item数量与效率的关系。</li>
<li>测试OpenCL多commandqueue的性能</li>
</ol>
<h2 id="一、opencv-OpenCL"><a href="#一、opencv-OpenCL" class="headerlink" title="一、opencv+OpenCL"></a>一、opencv+OpenCL</h2><h3 id="1-1-编译opencv-OpenCL的sdk"><a href="#1-1-编译opencv-OpenCL的sdk" class="headerlink" title="1.1 编译opencv+OpenCL的sdk"></a>1.1 编译opencv+OpenCL的sdk</h3><p>　　KCF算法总使用了不少的opencv函数，开始想的是编译一个包含OpenCL的opencv的sdk，然后通过调用该sdk从而实现使用GPU加速算法的目的。编译opencv+OpenCL的sdk当时踩了不少坑，多番尝试之后，使用下面的命令是可以成功编译。分别下载opencv-3.4.6、android-ndk-r16b、opencv_contrib-3.4.6,在opencv中建build目录，运行下面命令，命令中使用一些路径相关的参数要根据环境适当修改。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON -DCMAKE_TOOLCHAIN_FILE=&quot;/home/xxx/code/mobile/third_party/ opencv-3.4.6/platforms/android/android.toolchain.cmake&quot; -DANDROID_NDK=&quot;/home/xxx/code/mobile/tools/android-ndk-r16b&quot; -DANDROID_SDK=&quot;/home/xxx/code/mobile/tools/android_sdk/tools&quot; -DANDROID_NATIVE_API_LEVEL=19 -DANDROID_ABI=&quot;arm64-v8a&quot; -DANDROID_ARM_NEON=TRUE -DANDROID_STL=gnustl_static -DCMAKE_BUILD_TYPE=Release -DOPENCV_EXTRA_MODULES_PATH=&quot;/home/xxx/code/mobile/third_party/opencv_contrib-3.4.6/modules&quot; -DCMAKE_INSTALL_PREFIX=&quot;/home/xxx/code/mobile/third_party/opencv-3.4.6/install_20190623_OpenCL&quot; -DBUILD_opencv_java=ON -DBUILD_ANDROID_PROJECTS=OFF -DBUILD_ANDROID_EXAMPLES=OFF -DBUILD_DOCS=OFF -DBUILD_PERF_TESTS=OFF -DBUILD_TESTS=OFF -DBUILD_FAT_JAVA_LIB=OFF -DWITH_OpenCL=ON -DWITH_CUDA=OFF -DWITH_MATLAB=OFF -DBUILD_opencv_aruco=OFF -DBUILD_opencv_calib3d=OFF -DBUILD_opencv_features2d=OFF .. </span><br></pre></td></tr></table></figure>
<h3 id="1-2-测试mat到umat的相互转换的性能"><a href="#1-2-测试mat到umat的相互转换的性能" class="headerlink" title="1.2 测试mat到umat的相互转换的性能"></a>1.2 测试mat到umat的相互转换的性能</h3><p>　　在编译好opencv sdk之后，首先简单测试了一下sdk是否使用到了GPU资源。测试图片从CPU拷贝到GPU的的性能，opencv提供两组API。UMat::copyTo(OutputArray dst)和Mat::getMat(int access_flags)，实际测试中发现copyto那组性能比get的性能更好些，mat.getUmat函数会报错，还不知道什么原因。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">void testMatCopyToUmat(const char* img, int times) &#123;</span><br><span class="line">    cv::Mat image = cv::imread(img, cv::IMREAD_UNCHANGED);</span><br><span class="line">    cv::Mat out;</span><br><span class="line">    cv::UMat u_img;</span><br><span class="line">    if (u_img.empty())&#123;</span><br><span class="line">        //</span><br><span class="line">    &#125;</span><br><span class="line">    struct timeval start, end;</span><br><span class="line">    struct timeval last_time;</span><br><span class="line">    gettimeofday(&amp;start, NULL);</span><br><span class="line">    last_time = start;</span><br><span class="line">    for (int i = 0; i &lt; times; i++) &#123;</span><br><span class="line">        image.copyTo(u_img);</span><br><span class="line">        //cv::cvtColor(image, out, cv::COLOR_BGR2GRAY);</span><br><span class="line">        gettimeofday(&amp;end, NULL);</span><br><span class="line">        P(&quot;mat.copyToUmat:%d,run times:%d, spend:%d us&quot;, i,times, (end.tv_sec - last_time.tv_sec) * 1000000 + </span><br><span class="line">                                       (end.tv_usec - last_time.tv_usec));</span><br><span class="line">        last_time = end;</span><br><span class="line">    &#125;</span><br><span class="line">    gettimeofday(&amp;end, NULL);</span><br><span class="line">    P(&quot;mat.copyToUmat: run times:%d, spend:%d ms&quot;, times, (end.tv_sec - start.tv_sec) * 1000 + </span><br><span class="line">                                       (end.tv_usec - start.tv_usec)/1000);</span><br><span class="line">&#125;</span><br><span class="line">void testUMatCopyToMat(const char* img, int times) &#123;</span><br><span class="line">    cv::Mat image = cv::imread(img, cv::IMREAD_UNCHANGED);</span><br><span class="line">    cv::Mat out;</span><br><span class="line">    struct timeval start, end,last_time;</span><br><span class="line">    cv::UMat u_img;</span><br><span class="line">    image.copyTo(u_img);</span><br><span class="line"></span><br><span class="line">    gettimeofday(&amp;start, NULL);</span><br><span class="line">    last_time = start;</span><br><span class="line">    for (int i = 0; i &lt; times; i++) &#123;</span><br><span class="line">        u_img.copyTo(out);</span><br><span class="line">        gettimeofday(&amp;end, NULL);</span><br><span class="line">        P(&quot;mat.copyToUmat:%d,run times:%d, spend:%d us&quot;, i,times, (end.tv_sec - last_time.tv_sec) * 1000000 + </span><br><span class="line">                                       (end.tv_usec - last_time.tv_usec));</span><br><span class="line">        last_time = end;</span><br><span class="line">    &#125;</span><br><span class="line">    gettimeofday(&amp;end, NULL);</span><br><span class="line">    P(&quot;mat.copyToUmat: run times:%d, spend:%d ms&quot;, times, (end.tv_sec - start.tv_sec) * 1000 + </span><br><span class="line">                                       (end.tv_usec - start.tv_usec)/1000);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>| 手机型号 | CPU型号 | GPU型号 | OpenCL版本 | 首次mat拷贝umat | mat拷贝umat | 首次umat拷贝mat | umat拷贝mat | 图片格式 | 上行带宽 | 下行带宽 |<br>| —— | —— | —— | —— | —— | —— | —— | —— | —— | —— | —— | —— | —— | —— | —— |<br>|三星GALAXY On7|高通 骁龙410 MSM8916|	Adreno306|	2|	25.2ms|	0.8ms|	1.5ms|	0.8ms|	720<em>480 159KB|	运行1000次，221M&#x2F;s|	运行1000次，258M&#x2F;s|<br>|三星GALAXY On7|高通 骁龙410 MSM8916|	Adreno306|	2|	30.18ms|	2.88ms|	5.5ms|	2.9ms|	1920</em>1080 6MB|	运行1000次，2.14G&#x2F;s|	运行1000次，2.14G&#x2F;s|<br>|小米6 MI6|	骁龙 835|	高通 Adreno540|	2|16.602ms|	0.754ms|	2.85ms|	0.795ms|	1920<em>1080 6MB|	运行1000次，7.9G&#x2F;s|	运行1000次，8.06G&#x2F;s|<br>|小米6 MI6|	骁龙 835|	高通 Adreno540|	2|17.010ms|	0.332ms|	1ms|0.265ms|	720</em>480 159KB|	运行1000次，632M&#x2F;S|	运行1000次，898.2M&#x2F;s|	<br>|小米mix2s|	骁龙 845|	高通 Adreno630|	2|8.7ms|	2.1ms|	6.1ms|0.9ms|	1920<em>1080 6MB|	运行1000次，6.6G&#x2F;S|	运行1000次，6.62G&#x2F;s|	<br>|小米mix2s|	骁龙 845|	高通 Adreno630|	2|3.3ms|	0.5ms|	2.2ms|0.4ms|	720</em>480 1579KB|	运行1000次，654M&#x2F;S|	运行1000次，682M&#x2F;s|																</p>
<h3 id="1-3-测试OpenCL-cvtcolor函数性能"><a href="#1-3-测试OpenCL-cvtcolor函数性能" class="headerlink" title="1.3 测试OpenCL cvtcolor函数性能"></a>1.3 测试OpenCL cvtcolor函数性能</h3><p>　　在测试完CPU和GPU内存拷贝的性能之外，之后测试了cvtcolor函数的性能，由于动态加载，OpenCL函数首次加载时特别耗时，大概需要200ms。除此之外，在不同规格的图片上，OpenCL的计算性能大概是cpu的2到3倍。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">void cpu(const char* img, int times) &#123;</span><br><span class="line">    cv::Mat image; </span><br><span class="line">    cv::Mat out;</span><br><span class="line">    struct timeval start, end,last;</span><br><span class="line">    for (int i = 0; i &lt; times; i++) &#123;</span><br><span class="line">        image = cv::imread(img, cv::IMREAD_UNCHANGED);</span><br><span class="line">        gettimeofday(&amp;start, NULL);</span><br><span class="line">        cv::cvtColor(image, out, cv::COLOR_BGR2GRAY);</span><br><span class="line">        gettimeofday(&amp;end, NULL);</span><br><span class="line">        P(&quot;run times:%d, spend:%d us&quot;, i, (end.tv_sec - start.tv_sec) * 1000000 +</span><br><span class="line">                                       (end.tv_usec - start.tv_usec));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">void OpenCL(const char* img, int times) &#123;</span><br><span class="line">    cv::UMat u_img;</span><br><span class="line">    cv::Mat image; </span><br><span class="line">    cv::UMat out;</span><br><span class="line">    cv::Mat out1;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;cv::UMat&gt; v;</span><br><span class="line">    for(int i=0;i&lt;times;i++)&#123;</span><br><span class="line">      image = cv::imread(img, cv::IMREAD_UNCHANGED);</span><br><span class="line">      cv::UMat u_img;</span><br><span class="line">      image.copyTo(u_img);</span><br><span class="line">      v.push_back(u_img);</span><br><span class="line">    &#125;</span><br><span class="line">    struct timeval start, end,last;</span><br><span class="line">    for (int i = 0; i &lt; times; i++) &#123;</span><br><span class="line">        gettimeofday(&amp;start, NULL);</span><br><span class="line">        cv::cvtColor(v[i], out, cv::COLOR_BGR2GRAY);</span><br><span class="line">        gettimeofday(&amp;end, NULL);</span><br><span class="line">        P(&quot;run times:%d, spend:%d us&quot;, i, (end.tv_sec - start.tv_sec) * 1000000 +</span><br><span class="line">                                       (end.tv_usec - start.tv_usec));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>测试数据：</p>
<table>
<thead>
<tr>
<th>手机型号</th>
<th>cpu&#x2F;gpu</th>
<th>图片格式</th>
<th>首次运行时间</th>
<th>平均时间</th>
</tr>
</thead>
<tbody><tr>
<td>三星GALAXY On7</td>
<td>cpu</td>
<td>1920x1080</td>
<td>3.2ms</td>
<td>1.8ms</td>
</tr>
<tr>
<td>三星GALAXY On7</td>
<td>OpenCL</td>
<td>1920x1080</td>
<td>273ms</td>
<td>0.6ms</td>
</tr>
<tr>
<td>三星GALAXY On7</td>
<td>cpu</td>
<td>720x480</td>
<td>1.2ms</td>
<td>0.62ms</td>
</tr>
<tr>
<td>三星GALAXY On7</td>
<td>OpenCL</td>
<td>720x480</td>
<td>274ms</td>
<td>0.25ms</td>
</tr>
<tr>
<td>小米mix2s</td>
<td>cpu</td>
<td>1920x1080</td>
<td>3ms</td>
<td>1.3ms</td>
</tr>
<tr>
<td>小米mix2s</td>
<td>OpenCL</td>
<td>1920x1080</td>
<td>154ms</td>
<td>0.36ms</td>
</tr>
<tr>
<td>小米mix2s</td>
<td>cpu</td>
<td>720x480</td>
<td>0.5ms</td>
<td>0.21ms</td>
</tr>
<tr>
<td>小米mix2s</td>
<td>OpenCL</td>
<td>720x480</td>
<td>80.5ms</td>
<td>0.09ms</td>
</tr>
</tbody></table>
<h2 id="二、OpenCL核心API性能测试"><a href="#二、OpenCL核心API性能测试" class="headerlink" title="二、OpenCL核心API性能测试"></a>二、OpenCL核心API性能测试</h2><table>
<thead>
<tr>
<th>手机型号</th>
<th>cpux型号</th>
<th>GPU型号</th>
<th>OpenCL版本</th>
<th>API</th>
<th>测试数据</th>
</tr>
</thead>
<tbody><tr>
<td>小米6 MI6</td>
<td>骁龙 835</td>
<td>高通 Adreno540</td>
<td>2</td>
<td>gpu内存分配(clCreateBuffer)</td>
<td>1M 430us,5M 1000us,10M 2000us</td>
</tr>
<tr>
<td>小米6 MI6</td>
<td>骁龙 835</td>
<td>高通 Adreno540</td>
<td>2</td>
<td>cpu到gpu内存拷贝(writeBuffer)</td>
<td>1M 105us,5M 400us,10M 700us</td>
</tr>
<tr>
<td>小米6 MI6</td>
<td>骁龙 835</td>
<td>高通 Adreno540</td>
<td>2</td>
<td>gpu到cpu内存拷贝(ReadBuffer)</td>
<td>1M 60us,5M 400us,10M 600us</td>
</tr>
<tr>
<td>小米6 MI6</td>
<td>骁龙 835</td>
<td>高通 Adreno540</td>
<td>2</td>
<td>核函数编译clBuildProgram</td>
<td>69682 us</td>
</tr>
<tr>
<td>小米6 MI6</td>
<td>骁龙 835</td>
<td>高通 Adreno540</td>
<td>2</td>
<td>创建核对象clCreateKernel</td>
<td>50us</td>
</tr>
<tr>
<td>小米6 MI6</td>
<td>骁龙 835</td>
<td>高通 Adreno540</td>
<td>2</td>
<td>核函数clEnqueueNDRangeKernel</td>
<td>首次运行5000us，之后大概800us</td>
</tr>
</tbody></table>
<h2 id="三、-测试OpenCL-work-item数量与效率的关系"><a href="#三、-测试OpenCL-work-item数量与效率的关系" class="headerlink" title="三、 测试OpenCL work_item数量与效率的关系"></a>三、 测试OpenCL work_item数量与效率的关系</h2><p>　　在OpenCL编程中，work_item和work_group的设置对程序的性能有较大的影响。这里以内存拷贝为例测试OpenCL中work_item数量与效率的关系。通过一张3840x2160的图片拷贝，分别测试了CPU和GPU内存拷贝的性能，测试了在不同work_item条件下GPU内存拷贝性能的性能。从测试结果来看，不同work_item对opencl的性能有较大的影响。测试结果显示，最开始时work_item数量曾倍数关系，之后会在100ms抖动，最好的情况是work_item数量与bmpsize大小相同。测试机器为小米mix2s。</p>
<h3 id="3-1循环拷贝"><a href="#3-1循环拷贝" class="headerlink" title="3.1循环拷贝"></a>3.1循环拷贝</h3><p>bmpsize &#x3D; 3840x2160x3,运行时间13ms</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">char* out = new char[bmp_size];</span><br><span class="line"> for(int i=0;i&lt;bmp_size;i++)&#123;</span><br><span class="line">   // P(&quot;%d&quot;,i);</span><br><span class="line">   out[i] = bmp_data[i];</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2memcpy拷贝"><a href="#3-2memcpy拷贝" class="headerlink" title="3.2memcpy拷贝"></a>3.2memcpy拷贝</h3><p>bmpsize &#x3D; 3840x2160x3,运行时间3ms</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">memcpy(out,bmp_data,bmp_size);</span><br></pre></td></tr></table></figure>

<h3 id="3-3-opencl拷贝"><a href="#3-3-opencl拷贝" class="headerlink" title="3.3 opencl拷贝"></a>3.3 opencl拷贝</h3><p>核函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__kernel void convert_image(__global const uchar* in, </span><br><span class="line">            __global uchar* out，const int channel,</span><br><span class="line">            const int width, const int height)&#123;</span><br><span class="line">    int thread_count = get_global_size(0);</span><br><span class="line">    int size = width * height * channel;</span><br><span class="line">    int each_thread = size / thread_count;</span><br><span class="line">    int tid = get_global_id(0);</span><br><span class="line">    ; out[tid] = in[tid];</span><br><span class="line">    for(int i=tid*each_thread;i&lt;(tid+1)*each_thread;i++)&#123;</span><br><span class="line">        out[i] = in[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>关键代码与work_item的设置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">P(&quot;thread_count=%d&quot;, thread_count);</span><br><span class="line">gettimeofday(&amp;start,NULL);</span><br><span class="line">err = queue.enqueueNDRangeKernel(kernel, cl::NullRange, cl::NDRange(thread_count, 1),</span><br><span class="line">                           cl::NullRange, NULL, &amp;event);</span><br><span class="line">event.wait();</span><br><span class="line">gettimeofday(&amp;end,NULL);</span><br><span class="line">P(&quot;opecl wait:%d ms&quot;, (end.tv_sec - start.tv_sec) * 1000 + (end.tv_usec - start.tv_usec)/1000);</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>work_item数量</th>
<th>运行时间</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>2972ms</td>
</tr>
<tr>
<td>2</td>
<td>1526ms</td>
</tr>
<tr>
<td>4</td>
<td>792ms</td>
</tr>
<tr>
<td>8</td>
<td>418ms</td>
</tr>
<tr>
<td>16</td>
<td>252ms</td>
</tr>
<tr>
<td>32</td>
<td>166ms</td>
</tr>
<tr>
<td>64</td>
<td>122ms</td>
</tr>
<tr>
<td>128</td>
<td>104ms</td>
</tr>
<tr>
<td>256</td>
<td>64ms</td>
</tr>
<tr>
<td>512</td>
<td>60ms</td>
</tr>
<tr>
<td>1024</td>
<td>92ms</td>
</tr>
<tr>
<td>2048</td>
<td>662ms</td>
</tr>
<tr>
<td>4096</td>
<td>237ms</td>
</tr>
<tr>
<td>10240</td>
<td>180ms</td>
</tr>
<tr>
<td>102400</td>
<td>171ms</td>
</tr>
<tr>
<td>248832</td>
<td>167ms</td>
</tr>
<tr>
<td>2488320</td>
<td>16ms</td>
</tr>
<tr>
<td>24883200</td>
<td>15ms</td>
</tr>
</tbody></table>
<p><strong>疑问:当work_item为256或者512是个较好的值，但是不明白为什么2488320和24883200值效果会更好。</strong></p>
<h2 id="四、多commandqueue性能测试"><a href="#四、多commandqueue性能测试" class="headerlink" title="四、多commandqueue性能测试"></a>四、多commandqueue性能测试</h2><p>　　在学习和测试OpenCL的过程中，有一个疑问能否使用多个commandqueue来做任务的并行。假设有n个任务，每个任务包含CPU到GPU内存拷贝，核函数执行，和GPU到CPU的内存拷贝。分别测试了使用一个commandqueue和n个commandqueue的性能，测试结果显示多个commandqueue会比使用单个commandqueue性能略好一些，但是差别不大。除此之外，与work_item的设置有关，多个commandqueue可能比单个commandqueue性能性能提升15%。从GPU利用率来说，单个commandqueuGPU曲线呈锯齿形状，而多个commandque呈梯形。部分代码如下：<br>单个commandqueue：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">void test(const char* cl_file, const char* name, </span><br><span class="line">      const char* bmp_data, const int bmp_size, </span><br><span class="line">      const int width, const int height, const int channels,</span><br><span class="line">      const int line_size, const int thread_count,</span><br><span class="line">      const int run_times) </span><br><span class="line">&#123;</span><br><span class="line">  cl::Platform platforms = cl::Platform::getDefault();</span><br><span class="line">  //P(&quot;platform count:%d&quot;, platforms.size());</span><br><span class="line">  cl::Context context(CL_DEVICE_TYPE_GPU, NULL);</span><br><span class="line">  std::vector&lt;cl::Device&gt; devices = context.getInfo&lt;CL_CONTEXT_DEVICES&gt;();</span><br><span class="line">  P(&quot;Device count:%d&quot;, devices.size());</span><br><span class="line">  std::ifstream in(cl_file, std::ios::in);</span><br><span class="line">  std::stringstream buffer;</span><br><span class="line">  buffer &lt;&lt; in.rdbuf();</span><br><span class="line">  cl_int err = CL_SUCCESS;</span><br><span class="line">  cl::Program program_ = cl::Program(context, buffer.str());</span><br><span class="line">  err = program_.build(devices);</span><br><span class="line">  if (err != CL_SUCCESS) &#123;</span><br><span class="line">    P(&quot;build error&quot;);</span><br><span class="line">    return;</span><br><span class="line">  &#125;  </span><br><span class="line">  cl::Kernel kernel(program_, name, &amp;err);</span><br><span class="line">  if (err != CL_SUCCESS) &#123;</span><br><span class="line">    P(&quot;build error&quot;);</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  cl::CommandQueue queue(context, devices[0], 0, &amp;err);</span><br><span class="line">  if (err != CL_SUCCESS) &#123;</span><br><span class="line">    P(&quot;CommandQueue create error&quot;);</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  struct timeval start, end;</span><br><span class="line">  cl::Event event;</span><br><span class="line">  err = CL_SUCCESS;</span><br><span class="line">  for(int i = 0;i&lt;run_times;i++)&#123;</span><br><span class="line">  &#123;</span><br><span class="line">   </span><br><span class="line">    //see: https://github.khronos.org/OpenCL-CLHPP/classcl_1_1_buffer.html</span><br><span class="line">    cl::Buffer in_buf(context, CL_MEM_WRITE_ONLY, bmp_size);</span><br><span class="line">    cl::Buffer out_buf(context, CL_MEM_READ_ONLY, bmp_size);</span><br><span class="line">    err = queue.enqueueWriteBuffer(in_buf, true, 0, bmp_size, bmp_data, NULL, &amp;event);</span><br><span class="line"></span><br><span class="line">    kernel.setArg(0, in_buf);</span><br><span class="line">    kernel.setArg(1, out_buf);</span><br><span class="line">    kernel.setArg(2, line_size);</span><br><span class="line">    kernel.setArg(3, channels);</span><br><span class="line">    kernel.setArg(4, width);</span><br><span class="line">    kernel.setArg(5, height);</span><br><span class="line"></span><br><span class="line">    P(&quot;thread_count=%d&quot;, thread_count);</span><br><span class="line">    gettimeofday(&amp;start,NULL);</span><br><span class="line">    err = queue.enqueueNDRangeKernel(kernel, cl::NullRange, cl::NDRange(thread_count, 1),</span><br><span class="line">                               cl::NullRange, NULL, &amp;event);</span><br><span class="line">    event.wait();</span><br><span class="line">    gettimeofday(&amp;end,NULL);</span><br><span class="line">    P(&quot;opecl wait:%d ms&quot;, (end.tv_sec - start.tv_sec) * 1000 + </span><br><span class="line">                                         (end.tv_usec - start.tv_usec)/1000);</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">    char* h_out_buf = new char[bmp_size];</span><br><span class="line">    err = queue.enqueueReadBuffer(out_buf, true, 0, bmp_size, h_out_buf, NULL, &amp;event);</span><br><span class="line">    if(0!=memcmp(h_out_buf, bmp_data, bmp_size))&#123;</span><br><span class="line">      P(&quot;data not same&quot;);</span><br><span class="line">      return;</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">      P(&quot;data same&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>多个commandqueue：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">void test_mutil_command_queue(const char* cl_file, const char* name, </span><br><span class="line">      const char* bmp_data, int bmp_size, </span><br><span class="line">      const int width, const int height, const int channels,</span><br><span class="line">      const int line_size, const int thread_count,</span><br><span class="line">      const int run_times) </span><br><span class="line">&#123;</span><br><span class="line">  cl::Platform platforms = cl::Platform::getDefault();</span><br><span class="line">  //P(&quot;platform count:%d&quot;, platforms.size());</span><br><span class="line">  cl::Context context(CL_DEVICE_TYPE_GPU, NULL);</span><br><span class="line">  std::vector&lt;cl::Device&gt; devices = context.getInfo&lt;CL_CONTEXT_DEVICES&gt;();</span><br><span class="line">  P(&quot;Device count:%d&quot;, devices.size());</span><br><span class="line">  // cl::CommandQueue queue(context, devices[0], 0);</span><br><span class="line">  //</span><br><span class="line">  std::ifstream in(cl_file, std::ios::in);</span><br><span class="line">  std::stringstream buffer;</span><br><span class="line">  buffer &lt;&lt; in.rdbuf();</span><br><span class="line">  //</span><br><span class="line">  //cl::Program::Sources source&#123;</span><br><span class="line">  //    std::make_pair(buffer.str().c_str(), buffer.str().size()) &#125;;</span><br><span class="line">  cl_int err = CL_SUCCESS;</span><br><span class="line">  cl::Program program_ = cl::Program(context, buffer.str());</span><br><span class="line">  err = program_.build(devices);</span><br><span class="line">  if (err != CL_SUCCESS) &#123;</span><br><span class="line">    P(&quot;build error %d&quot;,err);</span><br><span class="line">    return;</span><br><span class="line">  &#125;  </span><br><span class="line"></span><br><span class="line">  struct timeval start, end,end2;</span><br><span class="line">  cl::Event event;</span><br><span class="line">  err = CL_SUCCESS;</span><br><span class="line">  gettimeofday(&amp;start, NULL);</span><br><span class="line">  std::vector&lt;cl::CommandQueue&gt; vQueue;</span><br><span class="line">  std::vector&lt;cl::Event&gt; vEvents;</span><br><span class="line">  std::vector&lt;cl::Buffer&gt; vInBuffers;</span><br><span class="line">  std::vector&lt;cl::Buffer&gt; vOutBuffers;</span><br><span class="line">  std::vector&lt;char*&gt; vHostOutBuf;</span><br><span class="line">  std::vector&lt;cl::Kernel&gt; vKernels;</span><br><span class="line">  std::vector&lt;char*&gt; vBmpdatas;</span><br><span class="line"></span><br><span class="line">  for(int i=0;i&lt;run_times;i++)&#123;</span><br><span class="line">    cl::Event event;</span><br><span class="line">    cl::CommandQueue queue(context, devices[0], 0, &amp;err);</span><br><span class="line">      if (err != CL_SUCCESS) &#123;</span><br><span class="line">      P(&quot;CommandQueue create error&quot;);</span><br><span class="line">      return;</span><br><span class="line">    &#125;</span><br><span class="line">    vQueue.push_back(queue);</span><br><span class="line">    vEvents.push_back(event);</span><br><span class="line">    cl::Buffer in_buf(context, CL_MEM_WRITE_ONLY, bmp_size);</span><br><span class="line">    cl::Buffer out_buf(context, CL_MEM_READ_ONLY, bmp_size);</span><br><span class="line">    vInBuffers.push_back(in_buf);</span><br><span class="line">    vOutBuffers.push_back(out_buf);</span><br><span class="line">    char* h_out_buf = new char[bmp_size];</span><br><span class="line">    vHostOutBuf.push_back(h_out_buf);</span><br><span class="line"></span><br><span class="line">    cl::Kernel kernel(program_, name, &amp;err);</span><br><span class="line">    if (err != CL_SUCCESS) &#123;</span><br><span class="line">      P(&quot;build error&quot;);</span><br><span class="line">      return;</span><br><span class="line">    &#125;</span><br><span class="line">    kernel.setArg(0, vInBuffers[i]);</span><br><span class="line">    kernel.setArg(1, vOutBuffers[i]);</span><br><span class="line">    kernel.setArg(2, line_size);</span><br><span class="line">    kernel.setArg(3, channels);</span><br><span class="line">    kernel.setArg(4, width);</span><br><span class="line">    kernel.setArg(5, height);</span><br><span class="line">    vKernels.push_back(kernel);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">  gettimeofday(&amp;end, NULL);</span><br><span class="line">  P(&quot;opecl create queue: spend:%d ms&quot;, (end.tv_sec - start.tv_sec) * 1000 + </span><br><span class="line">                                       (end.tv_usec - start.tv_usec)/1000);</span><br><span class="line"></span><br><span class="line">  for(int i=0;i&lt;run_times;i++)&#123;</span><br><span class="line">    err = vQueue[i].enqueueWriteBuffer(vInBuffers[i], false, 0, bmp_size, bmp_data, NULL, &amp;vEvents[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  for(int i=0;i&lt;run_times;i++)&#123;</span><br><span class="line">    vEvents[i].wait();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  for(int i=0;i&lt;run_times;i++)&#123;</span><br><span class="line">    err = vQueue[i].enqueueNDRangeKernel(vKernels[i], cl::NullRange, cl::NDRange(thread_count, 1),</span><br><span class="line">                                 cl::NullRange, NULL, &amp;vEvents[i]);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  for (int i = 0; i &lt; run_times; ++i)&#123; </span><br><span class="line">    vEvents[i].wait();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  for(int i=0;i&lt;run_times;i++)&#123;</span><br><span class="line">    err = vQueue[i].enqueueReadBuffer(vOutBuffers[i], false, 0, bmp_size, vHostOutBuf[i], NULL, &amp;vEvents[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  for(int i=0;i&lt;run_times;i++)&#123;</span><br><span class="line">    vEvents[i].wait();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  for(int i=0;i&lt;run_times;i++)&#123;</span><br><span class="line">    if (0!=memcmp(vHostOutBuf[i], bmp_data, bmp_size))&#123;</span><br><span class="line">      P(&quot;data not same&quot;);</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">      P(&quot;data same&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">wxquare</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
